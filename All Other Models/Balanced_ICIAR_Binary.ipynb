{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Taimoor_Datasets\\Paper 2\\WSI\\Processed_Class_Wise\\RESULTS_DIRECTORY_TRAIN_Dart_LUNG\\Code_TSNE\\Final\\Models Others\n",
      "Error: D:\\Taimoor_Datasets\\Paper 2\\WSI\\Processed_Class_Wise\\RESULTS_DIRECTORY_TRAIN_Dart_LUNG\\Code_TSNE\\Final\\Models Others/Results_ROC - The system cannot find the path specified.\n",
      "Libraries Loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 12396499203184480922, name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 15108276634\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 3105240546842923273\n",
       " physical_device_desc: \"device: 0, name: Quadro RTX 5000, pci bus id: 0000:17:00.0, compute capability: 7.5\"]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "work_directory_R = os.getcwd() + \"/Results_ROC\"\n",
    "work_directory_C = os.getcwd() + \"/Results_CM\"\n",
    "work_directory_P = os.getcwd() + \"/ResultsPredicted\"\n",
    "Results_File = os.getcwd() + \"/Metrics_Result.txt\"\n",
    "\n",
    "import shutil\n",
    "\n",
    "## Try to remove tree; if failed show an error using try...except on screen\n",
    "try:\n",
    "    shutil.rmtree(work_directory_R)\n",
    "    shutil.rmtree(work_directory_C)\n",
    "    shutil.rmtree(work_directory_P)\n",
    "    os.remove(Results_File)\n",
    "except OSError as e:\n",
    "    print (\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "    \n",
    "import gc as g\n",
    "\n",
    "# %reset\n",
    "from IPython import get_ipython\n",
    "get_ipython().magic('reset -sf') \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action='ignore',category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore',category=FutureWarning)\n",
    "\n",
    "\n",
    "import tensorflow as tf,keras\n",
    "from keras import backend as K, optimizers, regularizers\n",
    "# K.clear_session()\n",
    "\n",
    "import pickle, sys,os,cv2, numpy as np, pandas as pd, scipy, seaborn as sns, json, joblib\n",
    "from random import shuffle\n",
    "from keras.models import Sequential, Model, load_model, model_from_json\n",
    "from keras.models import Model\n",
    "from keras.layers import Lambda, LeakyReLU, multiply, Permute, Reshape, UpSampling2D\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPool2D, MaxPooling2D, GlobalAveragePooling2D, GlobalMaxPooling2D, Input, ZeroPadding2D, merge\n",
    "from keras.layers.merge import concatenate\n",
    "from keras import utils as np_utils\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras.optimizers import Adam,RMSprop,SGD\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import load_img,img_to_array,ImageDataGenerator,array_to_img\n",
    "from keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau, TensorBoard, EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "get_ipython().magic('matplotlib inline')\n",
    "%matplotlib inline\n",
    "\n",
    "from keras import layers\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score, confusion_matrix, roc_curve, auc, recall_score, precision_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "from itertools import cycle\n",
    "from six.moves import cPickle as pickle\n",
    "from scipy import interp\n",
    "from skimage.feature import local_binary_pattern,hog\n",
    "from skimage import data, exposure\n",
    "\n",
    "import itertools\n",
    "from time import time\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print('Libraries Loaded')\n",
    "\n",
    "tf.__version__\n",
    "tf.test.is_built_with_cuda()\n",
    "\n",
    "# GPU Check 1\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_pickle(data_file,q):\n",
    "    print('\\nTring to load pickle from %s' % data_file)\n",
    "    with open(data_file, 'rb') as file:\n",
    "        datasets = pickle.load(file)\n",
    "        dataset = datasets['dataset']\n",
    "\n",
    "    X_train = dataset['X_train']\n",
    "    Y_train = dataset['Y_train']\n",
    "    \n",
    "    print('\\nPickle Loaded Successfully!')\n",
    "    \n",
    "    del dataset\n",
    "\n",
    "    if q==1:\n",
    "        print('\\nX_train shape:', X_train.shape)\n",
    "        print('Y_train shape:', Y_train.shape)\n",
    "    \n",
    "    return X_train,Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_pickle(x_train,y_train, data_file):\n",
    "    print('\\nTrying to save pickle to %s' % data_file)\n",
    "    \n",
    "    X_train = x_train\n",
    "    Y_train = y_train\n",
    "    del x_train,y_train\n",
    "    \n",
    "    # creating dictionary to store trian and test data\n",
    "    datasets = {'dataset' : {'X_train': X_train,'Y_train': Y_train}}\n",
    "\n",
    "    with open(data_file, 'wb') as file:\n",
    "        pickle.dump(datasets, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        del datasets # to free up memory.\n",
    "        \n",
    "    print('\\nPickle Saved Successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resize_data(img_height,img_width,X_train_R,Y_train_R):\n",
    "    \n",
    "    Images_ = []\n",
    "    y_ = []    \n",
    "        \n",
    "    print('\\n\\n =============== Resizing Files =============== ')\n",
    "    for i in range(X_train_R.shape[0]):\n",
    "        image=X_train_R[i,:,:]\n",
    "        image=cv2.resize(image,(img_height,img_width))\n",
    "        Images_.append(image)\n",
    "        y_.append(Y_train_R[i])\n",
    "\n",
    "    Images_F = np.asarray(Images_) \n",
    "    Labels_F = np.asarray(y_)    \n",
    "    del Images_, y_\n",
    "\n",
    "    return (Images_F,Labels_F) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " DataSet-1 \n",
      "\n",
      "\n",
      "Tring to load pickle from PICKLESNEW/DataSet_ICIAR_AE_Train_B.pickle\n",
      "\n",
      "Pickle Loaded Successfully!\n",
      "\n",
      "X_train shape: (32215, 224, 224, 3)\n",
      "Y_train shape: (32215,)\n",
      "\n",
      "\n",
      "{0: 4970, 1: 27245}\n",
      "Class 0: 4970\n",
      "Class 1: 27245\n"
     ]
    }
   ],
   "source": [
    "option_D0a = 1\n",
    "\n",
    "img_height,img_width = 224,224\n",
    "\n",
    "if option_D0a==1:\n",
    "    print(\"\\n\\n\\n DataSet-1 \\n\")\n",
    "    path  = \"D://Taimoor_Datasets\\Paper 2\\WSI\\Processed_Class_Wise\\RESULTS_DIRECTORY_TRAIN_ICIAR/\"\n",
    "    \n",
    "    data_file = 'PICKLESNEW/DataSet_ICIAR_AE_Train_B.pickle'\n",
    "\n",
    "    num_classes=2\n",
    "    loss_a = 'binary_crossentropy'\n",
    "    \n",
    "    X_train_,Y_train_ = load_pickle(data_file,1)\n",
    "\n",
    "    # To train you need to resize the patches of any size user want.\n",
    "#     X_train_,Y_train_ = resize_data(img_height,img_width,X_train_,Y_train_)\n",
    "#     print(X_train_.shape)\n",
    "#     print(Y_train_.shape)\n",
    "    \n",
    "#     data_file_224 = 'PICKLESNEW/DataSet_ICIAR_AE_Train_B_224.pickle'   \n",
    "#     save_pickle(X_train_, Y_train_, data_file_224)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    unique, counts = np.unique(Y_train_, return_counts=True)\n",
    "    abc= dict(zip(unique, counts))\n",
    "    print(abc)\n",
    "    for i in range(len(abc)):\n",
    "        print('Class %d: %d' % (i, abc[i]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "x_train shape: (32215, 224, 224, 3)\n",
      "y_train shape: (32215,)\n",
      "\n",
      " Train Portion \n",
      "\n",
      "{0: 4970, 1: 27245}\n",
      "Class 0: 4970\n",
      "Class 1: 27245\n",
      "\n",
      "x_train shape: (10425, 224, 224, 3)\n",
      "y_train shape: (10425,)\n",
      "\n",
      " Train Portion \n",
      "\n",
      "{0: 4970, 1: 5455}\n",
      "Class 0: 4970\n",
      "Class 1: 5455\n"
     ]
    }
   ],
   "source": [
    "print('\\nx_train shape:', X_train_.shape)\n",
    "print('y_train shape:', Y_train_.shape)\n",
    "\n",
    "print(\"\\n Train Portion \\n\")\n",
    "unique, counts = np.unique(Y_train_, return_counts=True)\n",
    "abc= dict(zip(unique, counts))\n",
    "print(abc)\n",
    "for i in range(len(abc)):\n",
    "    print('Class %d: %d' % (i, abc[i]))\n",
    "\n",
    "index = range(5425,27215)\n",
    "X_train_ = np.delete(X_train_, index, axis=0)\n",
    "Y_train_ = np.delete(Y_train_, index)\n",
    "print('\\nx_train shape:', X_train_.shape)\n",
    "print('y_train shape:', Y_train_.shape)\n",
    "\n",
    "print(\"\\n Train Portion \\n\")\n",
    "unique, counts = np.unique(Y_train_, return_counts=True)\n",
    "abc= dict(zip(unique, counts))\n",
    "print(abc)\n",
    "for i in range(len(abc)):\n",
    "    print('Class %d: %d' % (i, abc[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: (8340, 224, 224, 3) , Train labels: (8340,)\n",
      "Test data: (2085, 224, 224, 3) , Test labels: (2085,)\n",
      "\n",
      " Train Portion \n",
      "\n",
      "{0: 3959, 1: 4381}\n",
      "Class 0: 3959\n",
      "Class 1: 4381\n",
      "\n",
      " Test Portion \n",
      "\n",
      "{0: 1011, 1: 1074}\n",
      "Class 0: 1011\n",
      "Class 1: 1074\n"
     ]
    }
   ],
   "source": [
    "if option_D0a==1:\n",
    "    size1 = 0.80    #D1\n",
    "#     size1 = 0.90    #D2\n",
    "    X_train, X_testA, Y_train, Y_testA = train_test_split(X_train_, Y_train_, train_size=size1, random_state=106,shuffle=True)\n",
    "    print('Train data:', X_train.shape,', Train labels:', Y_train.shape)\n",
    "    print('Test data:', X_testA.shape,', Test labels:', Y_testA.shape)\n",
    "    del X_train_, Y_train_\n",
    "    \n",
    "    print(\"\\n Train Portion \\n\")\n",
    "    unique, counts = np.unique(Y_train, return_counts=True)\n",
    "    abc= dict(zip(unique, counts))\n",
    "    print(abc)\n",
    "    for i in range(len(abc)):\n",
    "        print('Class %d: %d' % (i, abc[i]))\n",
    "\n",
    "    print(\"\\n Test Portion \\n\")\n",
    "    unique, counts = np.unique(Y_testA, return_counts=True)\n",
    "    abc= dict(zip(unique, counts))\n",
    "    print(abc)\n",
    "    for i in range(len(abc)):\n",
    "        print('Class %d: %d' % (i, abc[i]))    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: (8340, 224, 224, 3) , Train labels: (8340,)\n",
      "Test data: (2085, 224, 224, 3) , Test labels: (2085,)\n"
     ]
    }
   ],
   "source": [
    "X_traina, Y_traina = X_train,Y_train\n",
    "print('Train data:', X_traina.shape,', Train labels:', Y_traina.shape)\n",
    "\n",
    "X_testa, Y_testa = X_testA,Y_testA\n",
    "print('Test data:', X_testa.shape,', Test labels:', Y_testa.shape)\n",
    "\n",
    "X_testaB, Y_testaB = X_testA,Y_testA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_pickle_N(x_train, y_train, data_file):\n",
    "    print('\\nTrying to save pickle to %s' % data_file)\n",
    "    \n",
    "    X_train = x_train\n",
    "    Y_train = y_train\n",
    "    del x_train, y_train\n",
    "    \n",
    "    # creating dictionary to store trian and test data\n",
    "    datasets = {'dataset' : {'X_train': X_train, 'Y_train': Y_train}}\n",
    "\n",
    "    with open(data_file, 'wb') as file:\n",
    "        joblib.dump(datasets, file)\n",
    "\n",
    "        del datasets # to free up memory.\n",
    "    print('\\nPickle Saved Successfully!')\n",
    "    \n",
    "def load_pickle_N(data_file,q):\n",
    "\n",
    "    print('\\nTring to load pickle from %s' % data_file)\n",
    "    with open(data_file, 'rb') as file:\n",
    "        datasets = joblib.load(file)\n",
    "        dataset = datasets['dataset']\n",
    "\n",
    "    print('\\nPickle Loaded Successfully!')\n",
    "\n",
    "    X_train = dataset['X_train']\n",
    "    Y_train = dataset['Y_train']\n",
    "    del dataset\n",
    "\n",
    "    if q==1:\n",
    "        print('\\nX_train shape:', X_train.shape)\n",
    "        print('Y_train shape:', Y_train.shape)\n",
    "    \n",
    "    return X_train,Y_train    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Features\n"
     ]
    }
   ],
   "source": [
    "print('\\nTrain Features')\n",
    "\n",
    "work_directory_Pickles = os.getcwd() + \"/PICKLESNEW\"\n",
    "\n",
    "if option_D0a==1:\n",
    "    data_file_1 = '/AE_Train_ICIAR_R_B_N_224.pickle'\n",
    "\n",
    "data_file_R = work_directory_Pickles + data_file_1\n",
    "\n",
    "if not (os.path.exists(data_file_R)):\n",
    "    totalData = X_traina\n",
    "    totalData_Lables = Y_traina\n",
    "\n",
    "    X_traina = X_traina.astype('float32')/255. \n",
    "    \n",
    "    print('\\nRAW Features: ',X_traina.shape)\n",
    "    print('RAW Features Labels: ',Y_traina.shape)\n",
    "    save_pickle_N(X_traina, Y_traina, data_file_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Features\n"
     ]
    }
   ],
   "source": [
    "print('\\nTest Features')\n",
    "\n",
    "if option_D0a==1:\n",
    "    data_file_1 = '/AE_Test_ICIAR_R_B_N_224.pickle'\n",
    "\n",
    "data_file_R_T = work_directory_Pickles + data_file_1\n",
    "\n",
    "if not (os.path.exists(data_file_R_T)):\n",
    "    totalData = X_testa\n",
    "    totalData_Lables = Y_testa\n",
    "\n",
    "    X_testa = X_testa.astype('float32')/255. \n",
    "\n",
    "    print('\\nRAW Features: ',X_testa.shape)\n",
    "    print('RAW Features Labels: ',Y_testa.shape)\n",
    "    save_pickle_N(X_testa,Y_testa,data_file_R_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_cm(tl, pl, option_D0a,normalize, work_directory_C):\n",
    "    \n",
    "    if option_D0a==1:\n",
    "        classes = ['Non-Carcinoma' , 'Carcinoma'] # the same order as labels 0,1\n",
    "#         classes = ['B', 'IS', 'IN', 'N'] # the same order as labels 0,1,2,3\n",
    "    elif option_D0a==2:\n",
    "        classes = ['A', 'L', 'M', 'P', 'S'] # the same order as labels 0,1,2,3\n",
    "    \n",
    "    classesNames = classes    \n",
    "    confusion = confusion_matrix(tl, pl) #.astype(np.float)\n",
    "\n",
    "    if normalize:\n",
    "        print(\"Normalized confusion matrix\")\n",
    "        confusion = confusion.astype('float') / confusion.sum(axis=1)[:, np.newaxis]\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize=(15,10))\n",
    "\n",
    "    cm_sum = np.sum(confusion, axis=1, keepdims=True)\n",
    "    cm_perc = confusion / cm_sum.astype(float) * 100\n",
    "    annot = np.empty_like(confusion).astype(str)\n",
    "    nrows, ncols = confusion.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = confusion[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if i == j:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = '%.1f%%' % (p)\n",
    "            elif c == 0:\n",
    "                annot[i, j] = '%.1f%%' % (p)\n",
    "            else:\n",
    "                annot[i, j] = '%.1f%%' % (p)\n",
    "                                \n",
    "    cm = pd.DataFrame(confusion, index=classesNames, columns=classesNames)\n",
    "    cm.index.name = 'True Label'\n",
    "    cm.columns.name = 'Predicted Label'\n",
    "    rot = 45\n",
    "\n",
    "    ax.tick_params(axis=\"both\", pad=14, labelsize=0, length = 0)\n",
    "\n",
    "    res = sns.heatmap(cm, cmap= \"Blues\", annot=annot, fmt='', ax=ax, annot_kws={\"size\": 32}, cbar=False,\n",
    "                  linewidths=0.1, linecolor='gray')\n",
    "\n",
    "    res.set_xlabel(res.get_xlabel(), fontsize = 24, ha='center')\n",
    "    res.set_ylabel(res.get_ylabel(), fontsize = 24, ha='center')\n",
    "\n",
    "    res.set_xticklabels(res.get_xmajorticklabels(), fontsize = 24, ha='center')\n",
    "    res.set_yticklabels(res.get_ymajorticklabels(), fontsize = 24, ha='center')\n",
    "\n",
    "    plt.savefig(work_directory_C + '/Confusion_Matrix_D' + str(option_D0a), dpi=1000, bbox_inches=\"tight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\n",
      "\n",
      " ========================= MODEL ========================= \n",
      "\n",
      "Tring to load pickle from D:\\Taimoor_Datasets\\Paper 2\\WSI\\Processed_Class_Wise\\RESULTS_DIRECTORY_TRAIN_Dart_LUNG\\Code_TSNE\\Final\\Models Others/PICKLESNEW/AE_Train_ICIAR_R_B_N_224.pickle\n",
      "\n",
      "Pickle Loaded Successfully!\n",
      "\n",
      "Tring to load pickle from D:\\Taimoor_Datasets\\Paper 2\\WSI\\Processed_Class_Wise\\RESULTS_DIRECTORY_TRAIN_Dart_LUNG\\Code_TSNE\\Final\\Models Others/PICKLESNEW/AE_Test_ICIAR_R_B_N_224.pickle\n",
      "\n",
      "Pickle Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.95,allow_growth = True)\n",
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False,gpu_options=gpu_options))\n",
    "K.set_session(sess)\n",
    "\n",
    "if option_D0a==1:\n",
    "    Path_S = os.path.join(os.getcwd() + '/Weights_ICIAR/Binary_Balanced')\n",
    "\n",
    "Epochs_C = 25\n",
    "learning_rate = 10e-5\n",
    "Optimizer_C=Adam(lr=learning_rate)\n",
    "\n",
    "Batch_Size = 64\n",
    "\n",
    "Path_S_1 = os.path.join(os.getcwd())\n",
    "\n",
    "\n",
    "# Instantiate the cross validator\n",
    "kfold_splits = 5\n",
    "# kfold_splits = 3\n",
    "monitor='val_acc'\n",
    "# monitor='val_loss'\n",
    "dropout=0.5\n",
    "\n",
    "patience=5\n",
    "min_lr=1e-5\n",
    "verbose=1\n",
    "factor=0.3\n",
    "mode='max'\n",
    "# mode='min'\n",
    "pooling = 'avg'\n",
    "\n",
    "print('\\t\\t\\t\\t\\n\\n ========================= MODEL ========================= ')\n",
    "Epochs = 200\n",
    "   \n",
    "#D1\n",
    "X_train_R,Y_train_R = load_pickle_N(data_file_R,2)\n",
    "\n",
    "#D1\n",
    "X_test_R_T,Y_test_R_T = load_pickle_N(data_file_R_T,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### ###### ###### Deep Learning Models ###### ###### ###### \n",
    "def Pooling(pooling,x):\n",
    "    if pooling == 'avg':\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "    elif pooling == 'max':\n",
    "        x = GlobalMaxPooling2D()(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### ###### ###### BreastNet ###### ###### ###### \n",
    "\n",
    "from keras.layers import Concatenate, Layer, DepthwiseConv2D, AveragePooling2D, InputSpec, Add\n",
    "from keras.utils import conv_utils\n",
    "\n",
    "def cbam_block(cbam_feature, ratio=8):\n",
    "    # Author: @kobiso (https://github.com/kobiso)\n",
    "\n",
    "    cbam_feature = channel_attention(cbam_feature, ratio)\n",
    "    cbam_feature = spatial_attention(cbam_feature)\n",
    "    return cbam_feature\n",
    "\n",
    "def channel_attention(input_feature, ratio=8):\n",
    "    \n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    channel = input_feature._keras_shape[channel_axis]\n",
    "    \n",
    "    shared_layer_one = Dense(channel//ratio,\n",
    "                             activation='relu',\n",
    "                             kernel_initializer='he_normal',\n",
    "                             use_bias=True,\n",
    "                             bias_initializer='zeros')\n",
    "    shared_layer_two = Dense(channel,\n",
    "                             kernel_initializer='he_normal',\n",
    "                             use_bias=True,\n",
    "                             bias_initializer='zeros')\n",
    "    \n",
    "    avg_pool = GlobalAveragePooling2D()(input_feature)    \n",
    "    avg_pool = Reshape((1,1,channel))(avg_pool)\n",
    "    \n",
    "    assert avg_pool.shape[1:] == (1,1,channel)\n",
    "    avg_pool = shared_layer_one(avg_pool)\n",
    "    assert avg_pool.shape[1:] == (1,1,channel//ratio)\n",
    "    avg_pool = shared_layer_two(avg_pool)\n",
    "    assert avg_pool.shape[1:] == (1,1,channel)\n",
    "    \n",
    "    max_pool = GlobalMaxPooling2D()(input_feature)\n",
    "    max_pool = Reshape((1,1,channel))(max_pool)\n",
    "    assert max_pool.shape[1:] == (1,1,channel)\n",
    "    max_pool = shared_layer_one(max_pool)\n",
    "    assert max_pool.shape[1:] == (1,1,channel//ratio)\n",
    "    max_pool = shared_layer_two(max_pool)\n",
    "    assert max_pool.shape[1:] == (1,1,channel)\n",
    "    \n",
    "    cbam_feature = Add()([avg_pool,max_pool])\n",
    "    cbam_feature = Activation('sigmoid')(cbam_feature)\n",
    "\n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "    \n",
    "    return multiply([input_feature, cbam_feature])\n",
    "\n",
    "\n",
    "def spatial_attention(input_feature):\n",
    "    kernel_size = 7\n",
    "    \n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        channel = input_feature.shape[1]\n",
    "        cbam_feature = Permute((2,3,1))(input_feature)\n",
    "    else:\n",
    "        channel = input_feature.shape[-1]\n",
    "        cbam_feature = input_feature\n",
    "    \n",
    "    avg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n",
    "    assert avg_pool.shape[-1] == 1\n",
    "    max_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n",
    "    assert max_pool.shape[-1] == 1\n",
    "    concat = Concatenate(axis=3)([avg_pool, max_pool])\n",
    "    assert concat.shape[-1] == 2\n",
    "    cbam_feature = Conv2D(filters = 1,\n",
    "                    kernel_size=kernel_size,\n",
    "                    strides=1,\n",
    "                    padding='same',\n",
    "                    activation='sigmoid',\n",
    "                    kernel_initializer='he_normal',\n",
    "                    use_bias=False)(concat)\t\n",
    "    assert cbam_feature.shape[-1] == 1\n",
    "    \n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "        \n",
    "    return multiply([input_feature, cbam_feature])\n",
    "\n",
    "\n",
    "def residual_block(y, nb_channels, _strides=(1, 1), _project_shortcut=False):\n",
    "    # Author: @mjdietzx (https://gist.github.com/mjdietzx)\n",
    "\n",
    "    shortcut = y\n",
    "\n",
    "    y = Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides, padding='same')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = LeakyReLU()(y)\n",
    "\n",
    "    y = Conv2D(nb_channels, kernel_size=(3, 3), strides=(1, 1), padding='same')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "\n",
    "    if _project_shortcut or _strides != (1, 1):\n",
    "        shortcut = Conv2D(nb_channels, kernel_size=(1, 1), strides=_strides, padding='same')(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "\n",
    "    y = Add()([shortcut, y])\n",
    "    y = LeakyReLU()(y)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def BreastNet(input_W=img_width, input_H= img_height, n_classes=4, dropout_=0.1):\n",
    "\n",
    "    \"\"\"\n",
    "    M. Togaçar, K.B. Özkurt, B. Ergen et al., BreastNet: A novel ˘\n",
    "    convolutional neural network model through histopathological images for the diagnosis of breast\n",
    "    cancer, Physica A (2019), doi: https://doi.org/10.1016/j.physa.2019.123592 .\n",
    "    \"\"\"    \n",
    "    init = Input((input_W,input_H,3))\n",
    "\n",
    "    init = Input((input_W,input_H,3))\n",
    "    x = Conv2D(32, (3, 3), activation=None, padding='same')(init) \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(32, (3, 3), activation=None, padding='same')(x) \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x1 = MaxPooling2D((2,2))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), activation=None, padding='same')(x1)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = cbam_block(x)\n",
    "    x = residual_block(x, 64)\n",
    "    x2 = MaxPooling2D((2,2))(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), activation=None, padding='same')(x2)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = cbam_block(x)\n",
    "    x = residual_block(x, 128)\n",
    "    x3 = MaxPooling2D((2,2))(x)\n",
    "        \n",
    "    ginp1 = UpSampling2D(size=(2, 2), interpolation='bilinear')(x1)\n",
    "    ginp2 = UpSampling2D(size=(4, 4), interpolation='bilinear')(x2)\n",
    "    ginp3 = UpSampling2D(size=(8, 8), interpolation='bilinear')(x3)\n",
    "    \n",
    "    hypercolumn = concatenate([ginp1, ginp2, ginp3]) \n",
    "    gap = GlobalAveragePooling2D()(hypercolumn)\n",
    "\n",
    "    x = Dense(256, activation=None)(gap)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dropout_)(x)\n",
    "    \n",
    "    x = Dense(256, activation=None)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    y = Dense(n_classes, activation=\"softmax\", name=\"BreastNet\")(x)\n",
    "   \n",
    "    model = Model(init, y)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### ###### ###### LiverNet ###### ###### ###### \n",
    "\n",
    "class BilinearUpsampling(Layer):\n",
    "\n",
    "    \"\"\"\n",
    "        Bilinear Upsampling Class\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, upsampling=(2, 2), data_format=None, **kwargs):\n",
    "\n",
    "        \"\"\"\n",
    "            Constructor of Bilinear-Upsampling\n",
    "        \"\"\"\n",
    "        \n",
    "        super(BilinearUpsampling, self).__init__(**kwargs)\n",
    "        self.data_format = K.normalize_data_format(data_format)\n",
    "        self.upsampling = conv_utils.normalize_tuple(upsampling, 2, 'size')\n",
    "        self.input_spec = InputSpec(ndim=4)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        height = self.upsampling[0] * \\\n",
    "            input_shape[1] if input_shape[1] is not None else None\n",
    "        width = self.upsampling[1] * \\\n",
    "            input_shape[2] if input_shape[2] is not None else None\n",
    "        return (input_shape[0],\n",
    "                height,\n",
    "                width,\n",
    "                input_shape[3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.image.resize_bilinear(inputs, (int(inputs.shape[1]*self.upsampling[0]),\n",
    "                                                   int(inputs.shape[2]*self.upsampling[1])))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'size': self.upsampling,\n",
    "                  'data_format': self.data_format}\n",
    "        base_config = super(BilinearUpsampling, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "def aspp(x,input_shape,out_stride):\n",
    "\n",
    "    \"\"\"\n",
    "        ASPP Block\n",
    "        \n",
    "        Arguments:\n",
    "        \n",
    "            x: input feature map to the ASPP block\n",
    "            input_shape: input shape of the feature map\n",
    "            out_stride: the output stride\n",
    "            \n",
    "        Returns: \n",
    "            \n",
    "            output feature map after processing\n",
    "    \"\"\"\n",
    "    \n",
    "    b0=Conv2D(256,(1,1),padding=\"same\",use_bias=False)(x)\n",
    "    b0=BatchNormalization()(b0)\n",
    "    b0=Activation(\"relu\")(b0)\n",
    "\n",
    "    b1=DepthwiseConv2D((3,3),dilation_rate=(2,2),padding=\"same\",use_bias=False)(x)\n",
    "    b1=BatchNormalization()(b1)\n",
    "    b1=Activation(\"relu\")(b1)\n",
    "    b1=Conv2D(256,(1,1),padding=\"same\",use_bias=False)(b1)\n",
    "    b1=BatchNormalization()(b1)\n",
    "    b1=Activation(\"relu\")(b1)\n",
    "\n",
    "    b2=DepthwiseConv2D((3,3),dilation_rate=(3,3),padding=\"same\",use_bias=False)(x)\n",
    "    b2=BatchNormalization()(b2)\n",
    "    b2=Activation(\"relu\")(b2)\n",
    "    b2=Conv2D(256,(1,1),padding=\"same\",use_bias=False)(b2)\n",
    "    b2=BatchNormalization()(b2)\n",
    "    b2=Activation(\"relu\")(b2)\t\n",
    "\n",
    "    b3=DepthwiseConv2D((3,3),dilation_rate=(4,4),padding=\"same\",use_bias=False)(x)\n",
    "    b3=BatchNormalization()(b3)\n",
    "    b3=Activation(\"relu\")(b3)\n",
    "    b3=Conv2D(256,(1,1),padding=\"same\",use_bias=False)(b3)\n",
    "    b3=BatchNormalization()(b3)\n",
    "    b3=Activation(\"relu\")(b3)\n",
    "\n",
    "    b5=DepthwiseConv2D((3,3),dilation_rate=(6,6),padding=\"same\",use_bias=False)(x)\n",
    "    b5=BatchNormalization()(b5)\n",
    "    b5=Activation(\"relu\")(b5)\n",
    "    b5=Conv2D(256,(1,1),padding=\"same\",use_bias=False)(b5)\n",
    "    b5=BatchNormalization()(b5)\n",
    "    b5=Activation(\"relu\")(b5)\n",
    "\n",
    "    # b5 = DepthwiseConv2D((3,3),dilation_rate=(6,6),padding=\"same\",use_bias=False)(x)\n",
    "\n",
    "    out_shape=int(input_shape[0]/out_stride)\n",
    "    b4=AveragePooling2D(pool_size=(out_shape,out_shape))(x)\n",
    "    b4=Conv2D(256,(1,1),padding=\"same\",use_bias=False)(b4)\n",
    "    b4=BatchNormalization()(b4)\n",
    "    b4=Activation(\"relu\")(b4)\n",
    "    b4=BilinearUpsampling((out_shape,out_shape))(b4)\n",
    "\n",
    "    x=Concatenate()([b4,b0,b1,b2,b3,b5])\n",
    "    return x\n",
    "\n",
    "def LiverNet(input_W=img_width, input_H= img_height, n_classes=4, dropout_=0.1):\n",
    "    \n",
    "    \"\"\"\n",
    "        Method to create the final model\n",
    "    \"\"\"\n",
    "    \n",
    "    init = Input((input_W,input_H,3))\n",
    "    x = Conv2D(32, (3, 3), activation=None, padding='same')(init) \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(32, (3, 3), activation=None, padding='same')(x) \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x1 = MaxPooling2D((2,2))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), activation=None, padding='same')(x1)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = cbam_block(x)\n",
    "    x = residual_block(x, 64)\n",
    "    x2 = MaxPooling2D((2,2))(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), activation=None, padding='same')(x2)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = cbam_block(x)\n",
    "    x = residual_block(x, 128)\n",
    "    x3 = MaxPooling2D((2,2))(x)\n",
    "    \n",
    "    x1_aspp = aspp(x1,(112,112),1)\n",
    "    x2_aspp = aspp(x2,(56,56),1)\n",
    "    x3_aspp = aspp(x3,(28,28),1)\n",
    "\n",
    "    ginp1 = UpSampling2D(size=(2, 2), interpolation='bilinear')(x1_aspp)\n",
    "    ginp2 = UpSampling2D(size=(4, 4), interpolation='bilinear')(x2_aspp)\n",
    "    ginp3 = UpSampling2D(size=(8, 8), interpolation='bilinear')(x3_aspp)\n",
    "    \n",
    "    hypercolumn = Concatenate()([ginp1, ginp2, ginp3]) \n",
    "    gap = GlobalAveragePooling2D()(hypercolumn)\n",
    "\n",
    "    x = Dense(256, activation=None)(gap)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dropout_)(x)\n",
    "    \n",
    "    x = Dense(256, activation=None)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    y = Dense(n_classes, activation=\"softmax\", name=\"LiverNet\")(x)\n",
    "   \n",
    "    model = Model(init, y)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### ###### ###### HCNNet ###### ###### ###### \n",
    "\n",
    "# Function for a inception module block\n",
    "def inception_module(layer_in, f1, f2, f3):\n",
    "    # 1x1 conv\n",
    "    conv1 = Conv2D(f1, (1,1), padding='same', activation='relu')(layer_in)\n",
    "    # 3x3 conv\n",
    "    conv3 = Conv2D(f2, (3,3), padding='same', activation='relu')(layer_in)\n",
    "    # 5x5 conv\n",
    "    conv5 = Conv2D(f3, (5,5), padding='same', activation='relu')(layer_in)\n",
    "    # 3x3 max pooling\n",
    "    pool = MaxPooling2D((3,3), strides=(1,1), padding='same')(layer_in)\n",
    "    # concatenate filters, assumes filters/channels last\n",
    "    layer_out = concatenate([conv1, conv3, conv5, pool], axis=-1)\n",
    "    return layer_out\n",
    "\n",
    "def HCNNet(input_W=img_width, input_H= img_height, n_classes=4, dropout_=0.1):\n",
    "    \n",
    "    # define model input\n",
    "    init = Input((input_W,input_H,3))\n",
    "    x = Conv2D(64, (3, 3), activation=None, padding='same')(init) \n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((3,3), strides=(1,1), padding='same')(x)\n",
    "\n",
    "    x = Conv2D(64, (3, 3), activation=None, padding='same')(x) \n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((3,3), strides=(1,1), padding='same')(x)\n",
    "\n",
    "    # add inception module\n",
    "    layer1 = inception_module(x, 64, 128, 32)\n",
    "    layer2 = inception_module(layer1, 128, 192, 96)\n",
    "    layer3 = inception_module(layer2, 192, 208, 48)\n",
    "    layer4 = inception_module(layer3, 160, 224, 64)\n",
    "    layer5 = inception_module(layer4, 128, 256, 64)\n",
    "    layer6 = inception_module(layer5, 112, 288, 64)\n",
    "    layer7 = inception_module(layer6, 256, 320, 128)\n",
    "\n",
    "    x_ = MaxPooling2D((3,3), strides=(1,1), padding='same')(layer7)\n",
    "\n",
    "    layer8 = inception_module(x_, 256, 320, 128)\n",
    "    layer9 = inception_module(layer8, 384, 384, 128)\n",
    "\n",
    "    x_1 = MaxPooling2D((7,7), padding='same')(layer9)\n",
    "\n",
    "    x_2 = Dense(1024, activation='relu')(Flatten()(x_1))\n",
    "\n",
    "    y = Dense(n_classes, activation=\"softmax\", name=\"HCNNet\")(x_2)\n",
    "\n",
    "    model = Model(init, y)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### ###### ###### RuneCNN ###### ###### ###### \n",
    "\n",
    "def encoder(input_img):\n",
    "    #Encoder\n",
    "    x = Conv2D(8, (3, 3), strides=1, activation='relu', padding='same')(input_img)\n",
    "    x = MaxPooling2D((2, 2), strides=1,  padding='same')(x)\n",
    "\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Conv2D(8, (3, 3), strides=2, activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=1, padding='same')(x)\n",
    "\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Conv2D(8, (3, 3), strides=2, activation='relu', padding='same')(x)\n",
    "\n",
    "    x = Dense(4096, activation='relu')(Flatten()(x))\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(2048, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    encoded = Dense(1024, activation='relu')(x)\n",
    "#     print(\"shape of encoded\", K.int_shape(encoded))\n",
    "\n",
    "    return encoded\n",
    "\n",
    "def classifier_C(encode,num_classes,dropout_):\n",
    "    layer_1 = Dense(256, activation='relu')(encode)\n",
    "    layer_2 = Dense(512, activation='relu')(layer_1)\n",
    "    \n",
    "    layer_1 = Dropout(dropout_)(layer_1)\n",
    "    output = Dense(num_classes, activation='softmax', name=\"RuneCNN\")(layer_2)\n",
    "    return output\n",
    "\n",
    "def RuneCNN(input_W=img_width, input_H= img_height, n_classes=4, dropout_=0.1):\n",
    "    \n",
    "    \"\"\"\n",
    "        Method to create the final model\n",
    "    \"\"\"\n",
    "    \n",
    "    input_img = Input((input_W,input_H,3))\n",
    "    encode = encoder(input_img)\n",
    "    model = Model(input_img,classifier_C(encode,n_classes,dropout_))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Select Model Option \n",
      "\n",
      "\n",
      " 1-ResNet \n",
      " 2-DensNet \n",
      " 3-Inception \n",
      " 4-MobileNet \n",
      " 5-RuneCNN  \n",
      " 6-BreastNet \n",
      " 7-LiverNet  \n",
      " 8-HCNNet \n",
      "7\n",
      "\n",
      "Loading Model \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import vgg16, inception_v3, resnet50, mobilenet, densenet\n",
    "\n",
    "print('\\nSelect Model Option \\n')\n",
    "option_DM = int(input(\"\\n 1-ResNet \\n 2-DensNet \\n 3-Inception \\n 4-MobileNet \\n 5-RuneCNN  \\n 6-BreastNet \\n 7-LiverNet  \\n 8-HCNNet \\n\"))\n",
    "while option_DM not in (1,2,3,4,5,6,7,8):\n",
    "    option_DM = int(input(\"\\n 1-ResNet \\n 2-DensNet \\n 3-Inception \\n 4-MobileNet \\n 5-RuneCNN  \\n 6-BreastNet \\n 7-LiverNet  \\n 8-HCNNet \\n\"))\n",
    "\n",
    "with tf.device(\"/device:GPU:0\"):\n",
    "    print('\\nLoading Model \\n')\n",
    "    if option_DM==1:\n",
    "        model = resnet50.ResNet50(weights='imagenet', include_top=False, input_shape= (img_height,img_width,3),classes=num_classes)\n",
    "        filepath = \"weights.best 5FOLD ResNet.hdf5\"\n",
    "\n",
    "        # ResNet\n",
    "        x = Pooling(pooling,model.output)\n",
    "        x = Dense(1024, activation=\"relu\", kernel_regularizer=l2(0.0001))(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "        x = Dense(512, activation=\"relu\", kernel_regularizer=l2(0.0001))(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "        Last_FC_Layer = Dense(num_classes, activation= 'softmax', name='Last_Output_Layer')(x)\n",
    "        \n",
    "    elif option_DM==2:\n",
    "        model = densenet.DenseNet201(weights='imagenet', include_top=False, input_shape= (img_height,img_width,3),classes=num_classes)\n",
    "        filepath = \"weights.best 5FOLD DenseNet.hdf5\"\n",
    "\n",
    "        # DensNet\n",
    "        x = Pooling(pooling,model.output)\n",
    "        x = Dense(1024, activation=\"relu\", kernel_regularizer=l2(0.0001))(x)\n",
    "        x = Dense(512, activation=\"relu\", kernel_regularizer=l2(0.0001))(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "        Last_FC_Layer = Dense(num_classes, activation= 'softmax', name='Last_Output_Layer')(x)\n",
    "        \n",
    "    elif option_DM==3:\n",
    "        model = inception_v3.InceptionV3(weights='imagenet', include_top=False, input_shape= (img_height,img_width,3),classes=num_classes)\n",
    "        filepath = \"weights.best 5FOLD Inception.hdf5\"\n",
    "\n",
    "        # InceptionNet\n",
    "        x = Pooling(pooling,model.output)\n",
    "        x = Dense(1024, activation=\"relu\", kernel_regularizer=l2(0.0001))(x)\n",
    "        x = Dense(512, activation=\"relu\", kernel_regularizer=l2(0.0001))(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "        Last_FC_Layer = Dense(num_classes, activation= 'softmax', name='Last_Output_Layer')(x)\n",
    "\n",
    "    elif option_DM==4:\n",
    "        model = mobilenet.MobileNet(weights='imagenet', include_top=False, input_shape= (img_height,img_width,3),classes=num_classes)\n",
    "        filepath = \"weights.best 5FOLD MobileNet.hdf5\"\n",
    "\n",
    "        # MobileNet\n",
    "        x = Pooling(pooling,model.output)\n",
    "        x = Dense(1024, activation=\"relu\", kernel_regularizer=l2(0.0001))(x)\n",
    "        x = Dense(512, activation=\"relu\", kernel_regularizer=l2(0.0001))(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "        Last_FC_Layer = Dense(num_classes, activation= 'softmax', name='Last_Output_Layer')(x)\n",
    "        \n",
    "    elif option_DM==5:\n",
    "        # RuneCNN\n",
    "        gpu_model = RuneCNN(img_width, img_height, num_classes, dropout)\n",
    "        filepath = \"weights.best 5FOLD RuneCNN.hdf5\"\n",
    "\n",
    "    elif option_DM==6:\n",
    "        # BreastNet\n",
    "        gpu_model = BreastNet(img_width, img_height, num_classes, dropout)\n",
    "        filepath = \"weights.best 5FOLD BreastNet.hdf5\"\n",
    "\n",
    "    elif option_DM==7:\n",
    "        # LiverNet\n",
    "        gpu_model = LiverNet(img_width, img_height, num_classes, dropout)\n",
    "        filepath = \"weights.best 5FOLD LiverNet.hdf5\"\n",
    "\n",
    "    elif option_DM==8:\n",
    "        gpu_model = HCNNet(img_width, img_height, num_classes, dropout)\n",
    "        filepath = \"weights.best 5FOLD HCNNet.hdf5\"\n",
    "        \n",
    "#     print('\\nModel Loaded \\n')\n",
    "\n",
    "if option_DM==1 or option_DM==2 or option_DM==3 or option_DM==4:\n",
    "    gpu_model = Model(inputs= model.input, outputs= Last_FC_Layer)\n",
    "    \n",
    "gpu_model.compile(loss=loss_a,optimizer=Optimizer_C,metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " =============== Classifier Parameters =============== \n",
      "\n",
      "Loss:  binary_crossentropy\n",
      "Optimizer:  <keras.optimizers.Adam object at 0x000001651E3B45C0>\n",
      "Batch Size:  64\n",
      "Epoch:  25\n",
      "\n",
      "\n",
      " =============== Train Data Dimensions =============== \n",
      "\n",
      "Features R:  (8340, 224, 224, 3)\n",
      "Feature Labels:  (8340,)\n",
      "\n",
      "\n",
      " =============== Test Data Dimensions =============== \n",
      "\n",
      "Features R:  (2085, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# ORIGINAL ONE\n",
    "print('\\n\\n =============== Classifier Parameters =============== ')\n",
    "print('\\nLoss: ', loss_a)\n",
    "print('Optimizer: ', Optimizer_C)\n",
    "print('Batch Size: ', Batch_Size)\n",
    "print('Epoch: ', Epochs_C)\n",
    "\n",
    "print('\\n\\n =============== Train Data Dimensions =============== ')\n",
    "\n",
    "X_trainS,Y_trainS = X_train_R, Y_train_R\n",
    "print('\\nFeatures R: ',X_train_R.shape)\n",
    "print('Feature Labels: ',Y_trainS.shape)\n",
    "\n",
    "print('\\n\\n =============== Test Data Dimensions =============== ')\n",
    "X_testS,Y_testS = X_test_R_T, Y_test_R_T\n",
    "print('\\nFeatures R: ',X_test_R_T.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\t\t =============== Model Summary =============== \n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 224, 224, 32) 896         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 224, 224, 32) 128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 224, 224, 32) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 224, 224, 32) 9248        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 224, 224, 32) 128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 224, 224, 32) 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 112, 112, 32) 0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 112, 112, 64) 18496       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 112, 112, 64) 256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 112, 112, 64) 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 64)           0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_2 (GlobalM (None, 64)           0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 1, 1, 64)     0           global_average_pooling2d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 1, 1, 64)     0           global_max_pooling2d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1, 1, 8)      520         reshape_3[0][0]                  \n",
      "                                                                 reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1, 1, 64)     576         dense_3[0][0]                    \n",
      "                                                                 dense_3[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 1, 1, 64)     0           dense_4[0][0]                    \n",
      "                                                                 dense_4[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 1, 1, 64)     0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 112, 112, 64) 0           activation_6[0][0]               \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 112, 112, 1)  0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 112, 112, 1)  0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 112, 112, 2)  0           lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 112, 112, 1)  98          concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 112, 112, 64) 0           multiply_1[0][0]                 \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 112, 112, 64) 36928       multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 112, 112, 64) 256         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 112, 112, 64) 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 112, 112, 64) 36928       leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 112, 112, 64) 256         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 112, 112, 64) 0           multiply_2[0][0]                 \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 112, 112, 64) 0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 56, 56, 64)   0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 56, 56, 128)  73856       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 56, 56, 128)  512         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 56, 56, 128)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glo (None, 128)          0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_3 (GlobalM (None, 128)          0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 1, 1, 128)    0           global_average_pooling2d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 1, 1, 128)    0           global_max_pooling2d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1, 1, 16)     2064        reshape_5[0][0]                  \n",
      "                                                                 reshape_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1, 1, 128)    2176        dense_5[0][0]                    \n",
      "                                                                 dense_5[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 1, 1, 128)    0           dense_6[0][0]                    \n",
      "                                                                 dense_6[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1, 1, 128)    0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 56, 56, 128)  0           activation_8[0][0]               \n",
      "                                                                 activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 56, 56, 1)    0           multiply_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 56, 56, 1)    0           multiply_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 56, 56, 2)    0           lambda_3[0][0]                   \n",
      "                                                                 lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 56, 56, 1)    98          concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_4 (Multiply)           (None, 56, 56, 128)  0           multiply_3[0][0]                 \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 56, 56, 128)  147584      multiply_4[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 56, 56, 128)  512         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 56, 56, 128)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 56, 56, 128)  147584      leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 56, 56, 128)  512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 56, 56, 128)  0           multiply_4[0][0]                 \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 56, 56, 128)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 28, 28, 128)  0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_1 (DepthwiseCo (None, 112, 112, 32) 288         max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_2 (DepthwiseCo (None, 112, 112, 32) 288         max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_3 (DepthwiseCo (None, 112, 112, 32) 288         max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_4 (DepthwiseCo (None, 112, 112, 32) 288         max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_5 (DepthwiseCo (None, 56, 56, 64)   576         max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_6 (DepthwiseCo (None, 56, 56, 64)   576         max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_7 (DepthwiseCo (None, 56, 56, 64)   576         max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_8 (DepthwiseCo (None, 56, 56, 64)   576         max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_9 (DepthwiseCo (None, 28, 28, 128)  1152        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_10 (DepthwiseC (None, 28, 28, 128)  1152        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_11 (DepthwiseC (None, 28, 28, 128)  1152        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_12 (DepthwiseC (None, 28, 28, 128)  1152        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 32)     0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 112, 112, 32) 128         depthwise_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 112, 112, 32) 128         depthwise_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 112, 112, 32) 128         depthwise_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 112, 112, 32) 128         depthwise_conv2d_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 1, 1, 64)     0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 56, 56, 64)   256         depthwise_conv2d_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 56, 56, 64)   256         depthwise_conv2d_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 56, 56, 64)   256         depthwise_conv2d_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 56, 56, 64)   256         depthwise_conv2d_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 1, 1, 128)    0           max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 28, 28, 128)  512         depthwise_conv2d_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 28, 28, 128)  512         depthwise_conv2d_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 28, 28, 128)  512         depthwise_conv2d_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 28, 28, 128)  512         depthwise_conv2d_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 1, 1, 256)    8192        average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 112, 112, 32) 0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 112, 112, 32) 0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 112, 112, 32) 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 112, 112, 32) 0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 1, 1, 256)    16384       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 56, 56, 64)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 56, 56, 64)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 56, 56, 64)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 56, 56, 64)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 1, 1, 256)    32768       average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 28, 28, 128)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 28, 28, 128)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 28, 28, 128)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 28, 28, 128)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 1, 1, 256)    1024        conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 112, 112, 256 8192        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 112, 112, 256 8192        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 112, 112, 256 8192        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 112, 112, 256 8192        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 112, 112, 256 8192        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 1, 1, 256)    1024        conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 56, 56, 256)  16384       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 56, 56, 256)  16384       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 56, 56, 256)  16384       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 56, 56, 256)  16384       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 56, 56, 256)  16384       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 1, 1, 256)    1024        conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 28, 28, 256)  32768       max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 28, 28, 256)  32768       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 28, 28, 256)  32768       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 28, 28, 256)  32768       activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 28, 28, 256)  32768       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 1, 1, 256)    0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 112, 112, 256 1024        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 112, 112, 256 1024        conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 112, 112, 256 1024        conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 112, 112, 256 1024        conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 112, 112, 256 1024        conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 1, 1, 256)    0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 56, 56, 256)  1024        conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 56, 56, 256)  1024        conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 56, 56, 256)  1024        conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 56, 56, 256)  1024        conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 56, 56, 256)  1024        conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 1, 1, 256)    0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 28, 28, 256)  1024        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 28, 28, 256)  1024        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 28, 28, 256)  1024        conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 28, 28, 256)  1024        conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 28, 28, 256)  1024        conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bilinear_upsampling_1 (Bilinear (None, 112, 112, 256 0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 112, 112, 256 0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 112, 112, 256 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 112, 112, 256 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 112, 112, 256 0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 112, 112, 256 0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "bilinear_upsampling_2 (Bilinear (None, 56, 56, 256)  0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 56, 56, 256)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 56, 56, 256)  0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 56, 56, 256)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 56, 56, 256)  0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 56, 56, 256)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "bilinear_upsampling_3 (Bilinear (None, 28, 28, 256)  0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 28, 28, 256)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 28, 28, 256)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 28, 28, 256)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 28, 28, 256)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 28, 28, 256)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 112, 112, 153 0           bilinear_upsampling_1[0][0]      \n",
      "                                                                 activation_10[0][0]              \n",
      "                                                                 activation_12[0][0]              \n",
      "                                                                 activation_14[0][0]              \n",
      "                                                                 activation_16[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 56, 56, 1536) 0           bilinear_upsampling_2[0][0]      \n",
      "                                                                 activation_20[0][0]              \n",
      "                                                                 activation_22[0][0]              \n",
      "                                                                 activation_24[0][0]              \n",
      "                                                                 activation_26[0][0]              \n",
      "                                                                 activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 28, 28, 1536) 0           bilinear_upsampling_3[0][0]      \n",
      "                                                                 activation_30[0][0]              \n",
      "                                                                 activation_32[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "                                                                 activation_36[0][0]              \n",
      "                                                                 activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 224, 224, 153 0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 224, 224, 153 0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 224, 224, 153 0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 224, 224, 460 0           up_sampling2d_1[0][0]            \n",
      "                                                                 up_sampling2d_2[0][0]            \n",
      "                                                                 up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 4608)         0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 256)          1179904     global_average_pooling2d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 256)          1024        dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 256)          0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 256)          65792       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 256)          1024        dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 256)          0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "LiverNet (Dense)                (None, 2)            514         activation_41[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,102,014\n",
      "Trainable params: 2,088,702\n",
      "Non-trainable params: 13,312\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n\\t\\t =============== Model Summary =============== ')\n",
    "print(gpu_model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_history = []  \n",
    "cv_scores = []  \n",
    "\n",
    "skf = StratifiedKFold(n_splits=kfold_splits, shuffle=True)\n",
    "\n",
    "# Learning Rate Reducer\n",
    "learn_control = ReduceLROnPlateau(monitor=monitor, patience= patience,verbose=verbose,factor=factor, min_lr=min_lr)\n",
    "\n",
    "if not (os.path.exists(filepath)):\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor=monitor, verbose=verbose, save_best_only=True, mode=mode)\n",
    "    print('\\nNot Using real-time data augmentation.')\n",
    "    print('Training Starts...')      \n",
    "    print('X_train Concatenate shape:', X_trainS.shape)\n",
    "    print('Y_train Concatenate shape:', Y_trainS.shape)                                                                                                                                                                      \n",
    "\n",
    "    for index, (train_indices, val_indices) in enumerate(skf.split(X_trainS,Y_trainS)):\n",
    "        print(\"\\n\\n\\tTraining on fold \" + str(index+1) + \"/\" + str(kfold_splits) + \"...\")\n",
    "        xtrain_R, xvalid_R = X_trainS[train_indices], X_trainS[val_indices]\n",
    "        ytrain_R, yvalid_R = Y_trainS[train_indices], Y_trainS[val_indices]    \n",
    "\n",
    "        ytrain = tf.keras.utils.to_categorical(ytrain_R, num_classes)\n",
    "        yvalid = tf.keras.utils.to_categorical(yvalid_R, num_classes)        \n",
    "\n",
    "        print('\\nAfter Applying Categorical Opeartion')\n",
    "        print('Train data:', xtrain_R.shape,', Train labels:', ytrain.shape)\n",
    "        print('Validation data:', xvalid_R.shape,', Validation labels:', yvalid.shape)\n",
    "\n",
    "#                     print(\"Mode: \", Mode)\n",
    "        with tf.device('/device:GPU:0'):\n",
    "            history = gpu_model.fit(xtrain_R, ytrain, batch_size=Batch_Size, epochs=Epochs_C, validation_data=(xvalid_R, yvalid), verbose=1, callbacks=[learn_control, checkpoint])\n",
    "        model_history.append(history)    \n",
    "        print('\\nEvaluating the Model...')\n",
    "        scores = gpu_model.evaluate(xvalid_R, yvalid,verbose=0)\n",
    "        print(\"%s: %.2f%%\" % (gpu_model.metrics_names[1], scores[1]*100))\n",
    "        cv_scores.append(scores[1] * 100)\n",
    "\n",
    "        del xtrain_R, xvalid_R, ytrain, yvalid, ytrain_R, yvalid_R     \n",
    "\n",
    "    print(\"\\n%s: %.2f%%\" % (\"Mean Accuracy: \",np.mean(cv_scores)))\n",
    "    print(\"%s: %.2f%%\" % (\"Standard Deviation: +/-\", np.std(cv_scores)))\n",
    "    print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cv_scores), np.std(cv_scores)))\n",
    "    print('Estimated Accuracy %.3f (%.3f)' % (np.mean(cv_scores), np.std(cv_scores)))\n",
    "\n",
    "else:\n",
    "# load the models\n",
    "    print('\\n Classifier Loading Saved Models')\n",
    "    print('Model : ',filepath)\n",
    "    gpu_model = load_model(filepath)\n",
    "\n",
    "print(\"\\n\")                \n",
    "Y_testa_Old = Y_testS\n",
    "print('Before Categorical Features : ',Y_testa_Old.shape)\n",
    "\n",
    "Y_testS_ = tf.keras.utils.to_categorical(Y_testS, num_classes)      \n",
    "print('After Categorical Features : ',Y_testS_.shape)\n",
    "\n",
    "test_eval = gpu_model.evaluate(X_test_R_T,Y_testS_, verbose=0)\n",
    "\n",
    "print('\\n\\n =============== Overall Average Results =============== ')\n",
    "print('\\nAccuracy:', test_eval[1])\n",
    "\n",
    "#Predicted Labels\n",
    "predicted_classes = gpu_model.predict(X_test_R_T)\n",
    "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
    "test_labels = Y_testS\n",
    "\n",
    "print(\"Sensitivity = \", recall_score(test_labels, predicted_classes, average='weighted'))\n",
    "print(\"Precision = \", precision_score(test_labels, predicted_classes, average='weighted'))\n",
    "print(\"F1 = \", f1_score(test_labels, predicted_classes, average='weighted'))\n",
    "\n",
    "\n",
    "print('\\n\\n =============== Classwise Results =============== ')\n",
    "cmat = confusion_matrix(test_labels, predicted_classes) \n",
    "print(\"\\nAccuracy = \", cmat.diagonal()/cmat.sum(axis=1))\n",
    "print(\"Sensitivity = \", recall_score(test_labels, predicted_classes, average=None))\n",
    "print(\"Precision = \", precision_score(test_labels, predicted_classes, average=None))\n",
    "print(\"F1 = \", f1_score(test_labels, predicted_classes, average=None))\n",
    "\n",
    "\n",
    "print(\"\\n\\n\")            \n",
    "target_names = [\"Class {}\".format(ia) for ia in range(num_classes)]\n",
    "print(classification_report(test_labels, predicted_classes, target_names=target_names))\n",
    "\n",
    "\n",
    "with open(Path_S_1 + \"/Metrics_Result Others ICAIR.txt\", \"a\") as myfile:\n",
    "\n",
    "    myfile.write(\"\\n\\n =============== Dataset_\" + str(option_D0a) + \"=============== \")\n",
    "#     myfile.write(\"\\nAE Configuration: No. of Layers = \" + str(Number_of_Hidden_Layers_Input) + \", No. of Neurons = \" + str(Number_of_Neurons_Input) + \", Activation No = \" + str(Activation_Function_Input) + \"\\n\")                    \n",
    "    myfile.write(\"\\n =============== Overall Average Results =============== \")\n",
    "    myfile.write(\"\\nAccuracy: \" + str(\"%.5f\" % test_eval[1]))\n",
    "    myfile.write(\"\\nLoss: \" + str(\"%.5f\" % test_eval[0]))\n",
    "    myfile.write(\"\\nSensitivity: \" + str(recall_score(test_labels, predicted_classes, average='weighted')))\n",
    "    myfile.write(\"\\nPrecision: \" + str(precision_score(test_labels, predicted_classes, average='weighted')))\n",
    "    myfile.write(\"\\nF-1_Score: \" + str(f1_score(test_labels, predicted_classes, average='weighted')) + \"\\n\")\n",
    "\n",
    "    myfile.write(\"\\n =============== Classwise Average Results =============== \")\n",
    "    myfile.write(\"\\nAccuracy: \" + str(cmat.diagonal()/cmat.sum(axis=1)))\n",
    "    myfile.write(\"\\nSensitivity: \" + str(recall_score(test_labels, predicted_classes, average=None)))\n",
    "    myfile.write(\"\\nPrecision: \" + str(precision_score(test_labels, predicted_classes, average=None)))\n",
    "    myfile.write(\"\\nF-1_Score: \" + str(f1_score(test_labels, predicted_classes, average=None)) + \"\\n\")\n",
    "\n",
    "columns = 16\n",
    "work_directory_P = os.getcwd() + \"/ResultsPredicted\"\n",
    "\n",
    "if not os.path.exists(work_directory_P):\n",
    "    os.mkdir(work_directory_P)\n",
    "\n",
    "fig=plt.figure(figsize=(8, 8))\n",
    "\n",
    "left  = 0.125  # the left side of the subplots of the figure\n",
    "right = 1    # the right side of the subplots of the figure\n",
    "bottom = 0.1   # the bottom of the subplots of the figure\n",
    "top = 0.9      # the top of the subplots of the figure\n",
    "wspace = 0.25   # the amount of width reserved for blank space between subplots\n",
    "hspace = 0.25   # the amount of height reserved for white space between subplots\n",
    "\n",
    "plt.subplots_adjust( left, bottom, right, top, wspace, hspace)\n",
    "columns = 5\n",
    "rows = 5\n",
    "f = 12\n",
    "print('\\n\\n =============== Correct Predicted Samples =============== ')\n",
    "\n",
    "correct = np.where(predicted_classes==test_labels)[0]\n",
    "print(\"\\nFound %d correct labels\" % len(correct))\n",
    "for i1, correct in enumerate(correct[:rows*columns]):\n",
    "    ax = fig.add_subplot(rows, columns, i1+1)\n",
    "    plt.imshow(X_testaB[correct].astype('uint8'))\n",
    "    plt.axis('off')        \n",
    "    ax.set_title(\"Predict {}, Class {}\".format(predicted_classes[correct], test_labels[correct]), fontsize=f)\n",
    "    plt.imsave(work_directory_P+'/Correc_D-'+ str(option_D0a) + '_PL-' + str(predicted_classes[correct]) + '_TL-' + str(test_labels[correct]) + '_I-' + str(i1) + '.jpg', X_testaB[correct].astype('uint8'), dpi=fig.dpi)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig=plt.figure(figsize=(8, 8))\n",
    "plt.subplots_adjust( left, bottom, right, top, wspace, hspace)\n",
    "\n",
    "print('\\n\\n =============== Incorrect Predicted Samples =============== ')\n",
    "\n",
    "incorrect = np.where(predicted_classes!=test_labels)[0]\n",
    "print(\"\\nFound %d incorrect labels\" % len(incorrect))\n",
    "for i2, incorrect in enumerate(incorrect[:rows*columns]):\n",
    "    ax = fig.add_subplot(rows, columns, i2+1)\n",
    "    plt.imshow(X_testaB[incorrect].astype('uint8'))\n",
    "    plt.axis('off')        \n",
    "    ax.set_title(\"Predict {}, Class {}\".format(predicted_classes[incorrect], test_labels[incorrect]), fontsize=f)\n",
    "    plt.imsave(work_directory_P+'/Incorrect_D-'+ str(option_D0a) + '_PL-' + str(predicted_classes[incorrect]) + '_TL-' + str(test_labels[incorrect]) + '_I-' + str(i2) + '.jpg', X_testaB[incorrect].astype('uint8'), dpi=fig.dpi)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\n\\n ===============  Results =============== ')\n",
    "\n",
    "y_score = gpu_model.predict([X_test_R_T,X_test_H_T,X_test_L_T])\n",
    "y_test = tf.keras.utils.to_categorical(Y_testS, num_classes)      \n",
    "print(y_score.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# Plot linewidth.\n",
    "lw = 1\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for ia in range(num_classes):\n",
    "    fpr[ia], tpr[ia], _ = roc_curve(y_test[:, ia], y_score[:, ia])\n",
    "    roc_auc[ia] = auc(fpr[ia], tpr[ia])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "all_fpr = np.unique(np.concatenate([fpr[ib] for ib in range(num_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for ic in range(num_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[ic], tpr[ic])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= num_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure(1)\n",
    "plt.style.use('default')    \n",
    "plt.rcParams.update({'font.size': 13})\n",
    "\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='Micro Average (Area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=3)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='Macro Average (Area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=3)\n",
    "\n",
    "\n",
    "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "colors = cycle(prop_cycle.by_key()['color'])\n",
    "for id, color in zip(range(num_classes), colors):\n",
    "    if id==0:\n",
    "        plt.plot(fpr[id], tpr[id], color=next(colors), lw=lw,\n",
    "                 label='Class {0} (Area = {1:0.2f})'\n",
    "                 ''.format('A', roc_auc[id]))\n",
    "    elif id==1:\n",
    "        plt.plot(fpr[id], tpr[id], color=next(colors), lw=lw,\n",
    "                 label='Class {0} (Area = {1:0.2f})'\n",
    "                 ''.format('L', roc_auc[id]))\n",
    "    elif id==2:\n",
    "        plt.plot(fpr[id], tpr[id], color=next(colors), lw=lw,\n",
    "                 label='Class {0} (Area = {1:0.2f})'\n",
    "                 ''.format('M', roc_auc[id]))\n",
    "    elif id==3:\n",
    "        plt.plot(fpr[id], tpr[id], color=next(colors), lw=lw,\n",
    "                 label='Class {0} (Area = {1:0.2f})'\n",
    "                 ''.format('P', roc_auc[id]))\n",
    "    elif id==4:\n",
    "        plt.plot(fpr[id], tpr[id], color=next(colors), lw=lw,\n",
    "                 label='Class {0} (Area = {1:0.2f})'\n",
    "                 ''.format('S', roc_auc[id]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([-0.10, 1.10])\n",
    "plt.ylim([-0.10, 1.10])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.75, 1.0))\n",
    "\n",
    "#             work_directory_R = os.getcwd() + \"/Results_ROC\"\n",
    "#             work_directory_C = os.getcwd() + \"/Results_CM\"\n",
    "\n",
    "#             if not os.path.exists(work_directory_R):\n",
    "#                 os.mkdir(work_directory_R)\n",
    "\n",
    "#             plt.savefig(work_directory_R + '/ROC_D' + str(option_D0a), dpi=1000, bbox_inches=\"tight\")\n",
    "#             plt.show()\n",
    "\n",
    "#             if not os.path.exists(work_directory_C):\n",
    "#                 os.mkdir(work_directory_C)\n",
    "\n",
    "#             normalize = False\n",
    "#             plot_cm(test_labels, predicted_classes, option_D0a, normalize, work_directory_C)\n",
    "\n",
    "del gpu_model, predicted_classes, test_labels\n",
    "# ORIGINAL CODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
