{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Downloads\\Diagnostics-main\\Diagnostics-main\\Other Models\n",
      "Error: C:\\Users\\Administrator\\Downloads\\Diagnostics-main\\Diagnostics-main\\Other Models/Results_ROC - The system cannot find the path specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries Loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 11184906920517402712, name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 15152644096\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 15569756227705564908\n",
       " physical_device_desc: \"device: 0, name: Quadro RTX 5000, pci bus id: 0000:17:00.0, compute capability: 7.5\"]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "work_directory_R = os.getcwd() + \"/Results_ROC\"\n",
    "work_directory_C = os.getcwd() + \"/Results_CM\"\n",
    "work_directory_P = os.getcwd() + \"/ResultsPredicted\"\n",
    "Results_File = os.getcwd() + \"/Metrics_Result.txt\"\n",
    "\n",
    "import shutil\n",
    "\n",
    "## Try to remove tree; if failed show an error using try...except on screen\n",
    "try:\n",
    "    shutil.rmtree(work_directory_R)\n",
    "    shutil.rmtree(work_directory_C)\n",
    "    shutil.rmtree(work_directory_P)\n",
    "    os.remove(Results_File)\n",
    "except OSError as e:\n",
    "    print (\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "    \n",
    "import gc as g\n",
    "\n",
    "# %reset\n",
    "from IPython import get_ipython\n",
    "get_ipython().magic('reset -sf') \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action='ignore',category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore',category=FutureWarning)\n",
    "\n",
    "import tensorflow as tf,keras\n",
    "from keras import backend as K, optimizers, regularizers\n",
    "# K.clear_session()\n",
    "\n",
    "import pickle, sys,os,cv2, numpy as np, pandas as pd, scipy, seaborn as sns, json, joblib\n",
    "from random import shuffle\n",
    "from keras.models import Sequential, Model, load_model, model_from_json\n",
    "from keras.models import Model\n",
    "from keras.layers import Lambda, LeakyReLU, multiply, Permute, Reshape, UpSampling2D\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPool2D, MaxPooling2D, GlobalAveragePooling2D, GlobalMaxPooling2D, Input, ZeroPadding2D, merge\n",
    "from keras.layers.merge import concatenate\n",
    "from keras import utils as np_utils\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras.optimizers import Adam,RMSprop,SGD\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import load_img,img_to_array,ImageDataGenerator,array_to_img\n",
    "from keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau, TensorBoard, EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "get_ipython().magic('matplotlib inline')\n",
    "%matplotlib inline\n",
    "\n",
    "from keras import layers\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score, confusion_matrix, roc_curve, auc, recall_score, precision_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "from itertools import cycle\n",
    "from six.moves import cPickle as pickle\n",
    "from scipy import interp\n",
    "from skimage.feature import local_binary_pattern,hog\n",
    "from skimage import data, exposure\n",
    "\n",
    "import itertools\n",
    "from time import time\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print('Libraries Loaded')\n",
    "\n",
    "tf.__version__\n",
    "tf.test.is_built_with_cuda()\n",
    "\n",
    "# GPU Check 1\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_pickle(data_file,q):\n",
    "    print('\\nTrying to load pickle from %s' % data_file)\n",
    "    with open(data_file, 'rb') as file:\n",
    "        datasets = pickle.load(file)\n",
    "        dataset = datasets['dataset']\n",
    "\n",
    "    X_train = dataset['X_train']\n",
    "    Y_train = dataset['Y_train']\n",
    "    \n",
    "    print('\\nPickle Loaded Successfully!')\n",
    "    \n",
    "    del dataset\n",
    "\n",
    "    if q==1:\n",
    "        print('\\nX_train shape:', X_train.shape)\n",
    "        print('Y_train shape:', Y_train.shape)\n",
    "    \n",
    "    return X_train,Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_pickle(x_train,y_train, data_file):\n",
    "    print('\\nTrying to save pickle to %s' % data_file)\n",
    "    \n",
    "    X_train = x_train\n",
    "    Y_train = y_train\n",
    "    del x_train,y_train\n",
    "    \n",
    "    # creating dictionary to store trian and test data\n",
    "    datasets = {'dataset' : {'X_train': X_train,'Y_train': Y_train}}\n",
    "\n",
    "    with open(data_file, 'wb') as file:\n",
    "        pickle.dump(datasets, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        del datasets # to free up memory.\n",
    "        \n",
    "    print('\\nPickle Saved Successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resize_data(img_height,img_width,X_train_R,Y_train_R):\n",
    "    \n",
    "    Images_ = []\n",
    "    y_ = []    \n",
    "        \n",
    "    print('\\n\\n =============== Resizing Files =============== ')\n",
    "    for i in range(X_train_R.shape[0]):\n",
    "        image=X_train_R[i,:,:]\n",
    "        image=cv2.resize(image,(img_height,img_width))\n",
    "        Images_.append(image)\n",
    "        y_.append(Y_train_R[i])\n",
    "\n",
    "    Images_F = np.asarray(Images_) \n",
    "    Labels_F = np.asarray(y_)    \n",
    "    del Images_, y_\n",
    "\n",
    "    return (Images_F,Labels_F) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " DataSet-2 \n",
      "\n",
      "\n",
      "Trying to load pickle from PICKLESNEW/DataSet_DART_AE_Train.pickle\n",
      "\n",
      "Pickle Loaded Successfully!\n",
      "\n",
      "X_train shape: (188470, 40, 40, 3)\n",
      "Y_train shape: (188470,)\n",
      "\n",
      "\n",
      "{0: 38611, 1: 39092, 2: 40349, 3: 32228, 4: 38190}\n",
      "Class 0: 38611\n",
      "Class 1: 39092\n",
      "Class 2: 40349\n",
      "Class 3: 32228\n",
      "Class 4: 38190\n"
     ]
    }
   ],
   "source": [
    "option_D0a = 2 \n",
    "\n",
    "img_height,img_width = 224,224\n",
    "\n",
    "if option_D0a==2:\n",
    "    print(\"\\n\\n\\n DataSet-2 \\n\")\n",
    "    path  = \"/media/taimoor/5a400cc5-1c39-4106-8e51-04298d442f56/Processed_Class_Wise/RESULTS_DIRECTORY_Dart_LUNG/Classes_Data_224/Filter2/Data_Final/\"\n",
    "\n",
    "    \n",
    "    data_file = 'PICKLESNEW/DataSet_DART_AE_Train.pickle'   \n",
    "\n",
    "    num_classes=5\n",
    "    loss_a = 'categorical_crossentropy'\n",
    "    \n",
    "    X_train_,Y_train_ = load_pickle(data_file,1)\n",
    "\n",
    "    # To train you need to resize the patches of any size user want.\n",
    "#     X_train_,Y_train_ = resize_data(img_height,img_width,X_train_,Y_train_)\n",
    "#     print(X_train_.shape)\n",
    "#     print(Y_train_.shape)\n",
    "    \n",
    "#     data_file_224 = 'PICKLESNEW/DataSet_DART_AE_Train_224.pickle'   \n",
    "#     save_pickle(X_train_, Y_train_, data_file_224)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    unique, counts = np.unique(Y_train_, return_counts=True)\n",
    "    abc= dict(zip(unique, counts))\n",
    "    print(abc)\n",
    "    for i in range(len(abc)):\n",
    "        print('Class %d: %d' % (i, abc[i]))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: (150776, 40, 40, 3) , Train labels: (150776,)\n",
      "Test data: (37694, 40, 40, 3) , Test labels: (37694,)\n",
      "\n",
      " Train Portion \n",
      "\n",
      "{0: 30920, 1: 31268, 2: 32300, 3: 25783, 4: 30505}\n",
      "Class 0: 30920\n",
      "Class 1: 31268\n",
      "Class 2: 32300\n",
      "Class 3: 25783\n",
      "Class 4: 30505\n",
      "\n",
      " Test Portion \n",
      "\n",
      "{0: 7691, 1: 7824, 2: 8049, 3: 6445, 4: 7685}\n",
      "Class 0: 7691\n",
      "Class 1: 7824\n",
      "Class 2: 8049\n",
      "Class 3: 6445\n",
      "Class 4: 7685\n"
     ]
    }
   ],
   "source": [
    "if option_D0a==2:\n",
    "    size1 = 0.80    #D1\n",
    "#     size1 = 0.90    #D2\n",
    "    X_train, X_testA, Y_train, Y_testA = train_test_split(X_train_, Y_train_, train_size=size1, random_state=106,shuffle=True)\n",
    "    print('Train data:', X_train.shape,', Train labels:', Y_train.shape)\n",
    "    print('Test data:', X_testA.shape,', Test labels:', Y_testA.shape)\n",
    "    del X_train_, Y_train_\n",
    "    \n",
    "    print(\"\\n Train Portion \\n\")\n",
    "    unique, counts = np.unique(Y_train, return_counts=True)\n",
    "    abc= dict(zip(unique, counts))\n",
    "    print(abc)\n",
    "    for i in range(len(abc)):\n",
    "        print('Class %d: %d' % (i, abc[i]))\n",
    "\n",
    "    print(\"\\n Test Portion \\n\")\n",
    "    unique, counts = np.unique(Y_testA, return_counts=True)\n",
    "    abc= dict(zip(unique, counts))\n",
    "    print(abc)\n",
    "    for i in range(len(abc)):\n",
    "        print('Class %d: %d' % (i, abc[i]))    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: (150776, 40, 40, 3) , Train labels: (150776,)\n",
      "Test data: (37694, 40, 40, 3) , Test labels: (37694,)\n"
     ]
    }
   ],
   "source": [
    "X_traina, Y_traina = X_train,Y_train\n",
    "print('Train data:', X_traina.shape,', Train labels:', Y_traina.shape)\n",
    "\n",
    "X_testa, Y_testa = X_testA,Y_testA\n",
    "print('Test data:', X_testa.shape,', Test labels:', Y_testa.shape)\n",
    "\n",
    "X_testaB, Y_testaB = X_testA,Y_testA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_pickle_N(x_train, y_train, data_file):\n",
    "    print('\\nTrying to save pickle to %s' % data_file)\n",
    "    \n",
    "    X_train = x_train\n",
    "    Y_train = y_train\n",
    "    del x_train, y_train\n",
    "    \n",
    "    # creating dictionary to store trian and test data\n",
    "    datasets = {'dataset' : {'X_train': X_train, 'Y_train': Y_train}}\n",
    "\n",
    "    with open(data_file, 'wb') as file:\n",
    "        joblib.dump(datasets, file)\n",
    "\n",
    "        del datasets # to free up memory.\n",
    "    print('\\nPickle Saved Successfully!')\n",
    "    \n",
    "    \n",
    "def load_pickle_N(data_file,q):\n",
    "\n",
    "    print('\\nTring to load pickle from %s' % data_file)\n",
    "    with open(data_file, 'rb') as file:\n",
    "        datasets = joblib.load(file)\n",
    "        dataset = datasets['dataset']\n",
    "\n",
    "    X_train = dataset['X_train']\n",
    "    Y_train = dataset['Y_train']\n",
    "    \n",
    "    print('\\nPickle Loaded Successfully!')\n",
    "    \n",
    "    del dataset\n",
    "\n",
    "    if q==1:\n",
    "        print('\\nX_train shape:', X_train.shape)\n",
    "        print('Y_train shape:', Y_train.shape)\n",
    "    \n",
    "    return X_train,Y_train    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Features\n"
     ]
    }
   ],
   "source": [
    "print('\\nTrain Features')\n",
    "\n",
    "work_directory_Pickles = os.getcwd() + \"/PICKLESNEW\"\n",
    "\n",
    "if option_D0a==2:\n",
    "    data_file_1 = '/AE_Train_DART_LUNG_R_224.pickle'\n",
    "\n",
    "data_file_R = work_directory_Pickles + data_file_1\n",
    "\n",
    "if not (os.path.exists(data_file_R)):\n",
    "    totalData = X_traina\n",
    "    totalData_Lables = Y_traina\n",
    "\n",
    "    X_traina = X_traina.astype('float32')/255. \n",
    "    \n",
    "    print('\\nRAW Features: ',X_traina.shape)\n",
    "    print('RAW Features Labels: ',Y_traina.shape)\n",
    "    save_pickle_N(X_traina, Y_traina, data_file_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Features\n",
      "\n",
      "RAW Features:  (37694, 40, 40, 3)\n",
      "RAW Features Labels:  (37694,)\n",
      "\n",
      "Trying to save pickle to C:\\Users\\Administrator\\Downloads\\Diagnostics-main\\Diagnostics-main\\Other Models/PICKLESNEW/AE_Test_DART_LUNG_R_224.pickle\n",
      "\n",
      "Pickle Saved Successfully!\n"
     ]
    }
   ],
   "source": [
    "print('\\nTest Features')\n",
    "\n",
    "if option_D0a==2:\n",
    "    data_file_1 = '/AE_Test_DART_LUNG_R_224.pickle'\n",
    "\n",
    "data_file_R_T = work_directory_Pickles + data_file_1\n",
    "\n",
    "if not (os.path.exists(data_file_R_T)):\n",
    "    totalData = X_testa\n",
    "    totalData_Lables = Y_testa\n",
    "\n",
    "    X_testa = X_testa.astype('float32')/255. \n",
    "\n",
    "    print('\\nRAW Features: ',X_testa.shape)\n",
    "    print('RAW Features Labels: ',Y_testa.shape)\n",
    "    save_pickle_N(X_testa,Y_testa,data_file_R_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_cm(tl, pl, option_D0a,normalize, work_directory_C):\n",
    "    \n",
    "    if option_D0a==1:\n",
    "        classes = ['B', 'IS', 'IN', 'N'] # the same order as labels 0,1,2,3\n",
    "    elif option_D0a==2:\n",
    "        classes = ['A', 'L', 'M', 'P', 'S'] # the same order as labels 0,1,2,3\n",
    "    \n",
    "    classesNames = classes    \n",
    "    confusion = confusion_matrix(tl, pl) #.astype(np.float)\n",
    "\n",
    "    if normalize:\n",
    "        print(\"Normalized confusion matrix\")\n",
    "        confusion = confusion.astype('float') / confusion.sum(axis=1)[:, np.newaxis]\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize=(15,10))\n",
    "\n",
    "    cm_sum = np.sum(confusion, axis=1, keepdims=True)\n",
    "    cm_perc = confusion / cm_sum.astype(float) * 100\n",
    "    annot = np.empty_like(confusion).astype(str)\n",
    "    nrows, ncols = confusion.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = confusion[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if i == j:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = '%.1f%%' % (p)\n",
    "            elif c == 0:\n",
    "                annot[i, j] = '%.1f%%' % (p)\n",
    "            else:\n",
    "                annot[i, j] = '%.1f%%' % (p)\n",
    "                                \n",
    "    cm = pd.DataFrame(confusion, index=classesNames, columns=classesNames)\n",
    "    cm.index.name = 'True Label'\n",
    "    cm.columns.name = 'Predicted Label'\n",
    "    rot = 45\n",
    "\n",
    "    ax.tick_params(axis=\"both\", pad=14, labelsize=0, length = 0)\n",
    "\n",
    "    res = sns.heatmap(cm, cmap= \"Blues\", annot=annot, fmt='', ax=ax, annot_kws={\"size\": 32}, cbar=False,\n",
    "                  linewidths=0.1, linecolor='gray')\n",
    "\n",
    "    res.set_xlabel(res.get_xlabel(), fontsize = 24, ha='center')\n",
    "    res.set_ylabel(res.get_ylabel(), fontsize = 24, ha='center')\n",
    "\n",
    "    res.set_xticklabels(res.get_xmajorticklabels(), fontsize = 24, ha='center')\n",
    "    res.set_yticklabels(res.get_ymajorticklabels(), fontsize = 24, ha='center')\n",
    "\n",
    "    plt.savefig(work_directory_C + '/Confusion_Matrix_D' + str(option_D0a), dpi=1000, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\n",
      "\n",
      " ========================= MODEL ========================= \n",
      "\n",
      "Tring to load pickle from C:\\Users\\Administrator\\Downloads\\Diagnostics-main\\Diagnostics-main\\Other Models/PICKLESNEW/AE_Train_DART_LUNG_R_224.pickle\n",
      "\n",
      "Pickle Loaded Successfully!\n",
      "\n",
      "Tring to load pickle from C:\\Users\\Administrator\\Downloads\\Diagnostics-main\\Diagnostics-main\\Other Models/PICKLESNEW/AE_Test_DART_LUNG_R_224.pickle\n",
      "\n",
      "Pickle Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.95,allow_growth = True)\n",
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False,gpu_options=gpu_options))\n",
    "K.set_session(sess)\n",
    "\n",
    "if option_D0a==2:\n",
    "    Path_S = os.path.join(os.getcwd() + '/Weights_DART_LUNG')\n",
    "\n",
    "learning_rate = 10e-5\n",
    "Optimizer_C=Adam(lr=learning_rate)\n",
    "\n",
    "#D2\n",
    "Epochs_C = 30\n",
    "Batch_Size = 128\n",
    "# Instantiate the cross validator\n",
    "kfold_splits = 5\n",
    "dropout=0.5\n",
    "\n",
    "Path_S_1 = os.path.join(os.getcwd())\n",
    "\n",
    "weightsA = None\n",
    "\n",
    "\n",
    "monitor='val_acc'\n",
    "# monitor='val_loss'\n",
    "patience=5\n",
    "min_lr=1e-5\n",
    "verbose=1\n",
    "factor=0.3\n",
    "mode='max'\n",
    "# mode='min'\n",
    "pooling = 'avg'\n",
    "\n",
    "print('\\t\\t\\t\\t\\n\\n ========================= MODEL ========================= ')\n",
    "Epochs = 200\n",
    "#D2\n",
    "X_train_R,Y_train_R = load_pickle_N(data_file_R,2)\n",
    "\n",
    "#D2\n",
    "X_test_R_T,Y_test_R_T = load_pickle_N(data_file_R_T,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### ###### ###### Deep Learning Models ###### ###### ###### \n",
    "def Pooling(pooling,x):\n",
    "    if pooling == 'avg':\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "    elif pooling == 'max':\n",
    "        x = GlobalMaxPooling2D()(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### ###### ###### BreastNet ###### ###### ###### \n",
    "\n",
    "from keras.layers import Concatenate, Layer, DepthwiseConv2D, AveragePooling2D, InputSpec, Add\n",
    "from keras.utils import conv_utils\n",
    "\n",
    "def cbam_block(cbam_feature, ratio=8):\n",
    "    # Author: @kobiso (https://github.com/kobiso)\n",
    "\n",
    "    cbam_feature = channel_attention(cbam_feature, ratio)\n",
    "    cbam_feature = spatial_attention(cbam_feature)\n",
    "    return cbam_feature\n",
    "\n",
    "def channel_attention(input_feature, ratio=8):\n",
    "    \n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    channel = input_feature._keras_shape[channel_axis]\n",
    "    \n",
    "    shared_layer_one = Dense(channel//ratio,\n",
    "                             activation='relu',\n",
    "                             kernel_initializer='he_normal',\n",
    "                             use_bias=True,\n",
    "                             bias_initializer='zeros')\n",
    "    shared_layer_two = Dense(channel,\n",
    "                             kernel_initializer='he_normal',\n",
    "                             use_bias=True,\n",
    "                             bias_initializer='zeros')\n",
    "    \n",
    "    avg_pool = GlobalAveragePooling2D()(input_feature)    \n",
    "    avg_pool = Reshape((1,1,channel))(avg_pool)\n",
    "    \n",
    "    assert avg_pool.shape[1:] == (1,1,channel)\n",
    "    avg_pool = shared_layer_one(avg_pool)\n",
    "    assert avg_pool.shape[1:] == (1,1,channel//ratio)\n",
    "    avg_pool = shared_layer_two(avg_pool)\n",
    "    assert avg_pool.shape[1:] == (1,1,channel)\n",
    "    \n",
    "    max_pool = GlobalMaxPooling2D()(input_feature)\n",
    "    max_pool = Reshape((1,1,channel))(max_pool)\n",
    "    assert max_pool.shape[1:] == (1,1,channel)\n",
    "    max_pool = shared_layer_one(max_pool)\n",
    "    assert max_pool.shape[1:] == (1,1,channel//ratio)\n",
    "    max_pool = shared_layer_two(max_pool)\n",
    "    assert max_pool.shape[1:] == (1,1,channel)\n",
    "    \n",
    "    cbam_feature = Add()([avg_pool,max_pool])\n",
    "    cbam_feature = Activation('sigmoid')(cbam_feature)\n",
    "\n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "    \n",
    "    return multiply([input_feature, cbam_feature])\n",
    "\n",
    "\n",
    "def spatial_attention(input_feature):\n",
    "    kernel_size = 7\n",
    "    \n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        channel = input_feature.shape[1]\n",
    "        cbam_feature = Permute((2,3,1))(input_feature)\n",
    "    else:\n",
    "        channel = input_feature.shape[-1]\n",
    "        cbam_feature = input_feature\n",
    "    \n",
    "    avg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n",
    "    assert avg_pool.shape[-1] == 1\n",
    "    max_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n",
    "    assert max_pool.shape[-1] == 1\n",
    "    concat = Concatenate(axis=3)([avg_pool, max_pool])\n",
    "    assert concat.shape[-1] == 2\n",
    "    cbam_feature = Conv2D(filters = 1,\n",
    "                    kernel_size=kernel_size,\n",
    "                    strides=1,\n",
    "                    padding='same',\n",
    "                    activation='sigmoid',\n",
    "                    kernel_initializer='he_normal',\n",
    "                    use_bias=False)(concat)\t\n",
    "    assert cbam_feature.shape[-1] == 1\n",
    "    \n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "        \n",
    "    return multiply([input_feature, cbam_feature])\n",
    "\n",
    "\n",
    "def residual_block(y, nb_channels, _strides=(1, 1), _project_shortcut=False):\n",
    "    # Author: @mjdietzx (https://gist.github.com/mjdietzx)\n",
    "\n",
    "    shortcut = y\n",
    "\n",
    "    y = Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides, padding='same')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = LeakyReLU()(y)\n",
    "\n",
    "    y = Conv2D(nb_channels, kernel_size=(3, 3), strides=(1, 1), padding='same')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "\n",
    "    if _project_shortcut or _strides != (1, 1):\n",
    "        shortcut = Conv2D(nb_channels, kernel_size=(1, 1), strides=_strides, padding='same')(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "\n",
    "    y = Add()([shortcut, y])\n",
    "    y = LeakyReLU()(y)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def BreastNet(input_W=img_width, input_H= img_height, n_classes=4, dropout_=0.1):\n",
    "\n",
    "    \"\"\"\n",
    "    M. Togaçar, K.B. Özkurt, B. Ergen et al., BreastNet: A novel ˘\n",
    "    convolutional neural network model through histopathological images for the diagnosis of breast\n",
    "    cancer, Physica A (2019), doi: https://doi.org/10.1016/j.physa.2019.123592 .\n",
    "    \"\"\"    \n",
    "    init = Input((input_W,input_H,3))\n",
    "\n",
    "    init = Input((input_W,input_H,3))\n",
    "    x = Conv2D(32, (3, 3), activation=None, padding='same')(init) \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(32, (3, 3), activation=None, padding='same')(x) \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x1 = MaxPooling2D((2,2))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), activation=None, padding='same')(x1)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = cbam_block(x)\n",
    "    x = residual_block(x, 64)\n",
    "    x2 = MaxPooling2D((2,2))(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), activation=None, padding='same')(x2)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = cbam_block(x)\n",
    "    x = residual_block(x, 128)\n",
    "    x3 = MaxPooling2D((2,2))(x)\n",
    "        \n",
    "    ginp1 = UpSampling2D(size=(2, 2), interpolation='bilinear')(x1)\n",
    "    ginp2 = UpSampling2D(size=(4, 4), interpolation='bilinear')(x2)\n",
    "    ginp3 = UpSampling2D(size=(8, 8), interpolation='bilinear')(x3)\n",
    "    \n",
    "    hypercolumn = concatenate([ginp1, ginp2, ginp3]) \n",
    "    gap = GlobalAveragePooling2D()(hypercolumn)\n",
    "\n",
    "    x = Dense(256, activation=None)(gap)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dropout_)(x)\n",
    "    \n",
    "    x = Dense(256, activation=None)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    y = Dense(n_classes, activation=\"softmax\", name=\"BreastNet\")(x)\n",
    "   \n",
    "    model = Model(init, y)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### ###### ###### LiverNet ###### ###### ###### \n",
    "\n",
    "class BilinearUpsampling(Layer):\n",
    "\n",
    "    \"\"\"\n",
    "        Bilinear Upsampling Class\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, upsampling=(2, 2), data_format=None, **kwargs):\n",
    "\n",
    "        \"\"\"\n",
    "            Constructor of Bilinear-Upsampling\n",
    "        \"\"\"\n",
    "        \n",
    "        super(BilinearUpsampling, self).__init__(**kwargs)\n",
    "        self.data_format = K.normalize_data_format(data_format)\n",
    "        self.upsampling = conv_utils.normalize_tuple(upsampling, 2, 'size')\n",
    "        self.input_spec = InputSpec(ndim=4)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        height = self.upsampling[0] * \\\n",
    "            input_shape[1] if input_shape[1] is not None else None\n",
    "        width = self.upsampling[1] * \\\n",
    "            input_shape[2] if input_shape[2] is not None else None\n",
    "        return (input_shape[0],\n",
    "                height,\n",
    "                width,\n",
    "                input_shape[3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.image.resize_bilinear(inputs, (int(inputs.shape[1]*self.upsampling[0]),\n",
    "                                                   int(inputs.shape[2]*self.upsampling[1])))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'size': self.upsampling,\n",
    "                  'data_format': self.data_format}\n",
    "        base_config = super(BilinearUpsampling, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "def aspp(x,input_shape,out_stride):\n",
    "\n",
    "    \"\"\"\n",
    "        ASPP Block\n",
    "        \n",
    "        Arguments:\n",
    "        \n",
    "            x: input feature map to the ASPP block\n",
    "            input_shape: input shape of the feature map\n",
    "            out_stride: the output stride\n",
    "            \n",
    "        Returns: \n",
    "            \n",
    "            output feature map after processing\n",
    "    \"\"\"\n",
    "    \n",
    "    b0=Conv2D(256,(1,1),padding=\"same\",use_bias=False)(x)\n",
    "    b0=BatchNormalization()(b0)\n",
    "    b0=Activation(\"relu\")(b0)\n",
    "\n",
    "    b1=DepthwiseConv2D((3,3),dilation_rate=(2,2),padding=\"same\",use_bias=False)(x)\n",
    "    b1=BatchNormalization()(b1)\n",
    "    b1=Activation(\"relu\")(b1)\n",
    "    b1=Conv2D(256,(1,1),padding=\"same\",use_bias=False)(b1)\n",
    "    b1=BatchNormalization()(b1)\n",
    "    b1=Activation(\"relu\")(b1)\n",
    "\n",
    "    b2=DepthwiseConv2D((3,3),dilation_rate=(3,3),padding=\"same\",use_bias=False)(x)\n",
    "    b2=BatchNormalization()(b2)\n",
    "    b2=Activation(\"relu\")(b2)\n",
    "    b2=Conv2D(256,(1,1),padding=\"same\",use_bias=False)(b2)\n",
    "    b2=BatchNormalization()(b2)\n",
    "    b2=Activation(\"relu\")(b2)\t\n",
    "\n",
    "    b3=DepthwiseConv2D((3,3),dilation_rate=(4,4),padding=\"same\",use_bias=False)(x)\n",
    "    b3=BatchNormalization()(b3)\n",
    "    b3=Activation(\"relu\")(b3)\n",
    "    b3=Conv2D(256,(1,1),padding=\"same\",use_bias=False)(b3)\n",
    "    b3=BatchNormalization()(b3)\n",
    "    b3=Activation(\"relu\")(b3)\n",
    "\n",
    "    b5=DepthwiseConv2D((3,3),dilation_rate=(6,6),padding=\"same\",use_bias=False)(x)\n",
    "    b5=BatchNormalization()(b5)\n",
    "    b5=Activation(\"relu\")(b5)\n",
    "    b5=Conv2D(256,(1,1),padding=\"same\",use_bias=False)(b5)\n",
    "    b5=BatchNormalization()(b5)\n",
    "    b5=Activation(\"relu\")(b5)\n",
    "\n",
    "    # b5 = DepthwiseConv2D((3,3),dilation_rate=(6,6),padding=\"same\",use_bias=False)(x)\n",
    "\n",
    "    out_shape=int(input_shape[0]/out_stride)\n",
    "    b4=AveragePooling2D(pool_size=(out_shape,out_shape))(x)\n",
    "    b4=Conv2D(256,(1,1),padding=\"same\",use_bias=False)(b4)\n",
    "    b4=BatchNormalization()(b4)\n",
    "    b4=Activation(\"relu\")(b4)\n",
    "    b4=BilinearUpsampling((out_shape,out_shape))(b4)\n",
    "\n",
    "    x=Concatenate()([b4,b0,b1,b2,b3,b5])\n",
    "    return x\n",
    "\n",
    "def LiverNet(input_W=img_width, input_H= img_height, n_classes=4, dropout_=0.1):\n",
    "    \n",
    "    \"\"\"\n",
    "        Method to create the final model\n",
    "    \"\"\"\n",
    "    \n",
    "    init = Input((input_W,input_H,3))\n",
    "    x = Conv2D(32, (3, 3), activation=None, padding='same')(init) \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(32, (3, 3), activation=None, padding='same')(x) \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x1 = MaxPooling2D((2,2))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), activation=None, padding='same')(x1)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = cbam_block(x)\n",
    "    x = residual_block(x, 64)\n",
    "    x2 = MaxPooling2D((2,2))(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), activation=None, padding='same')(x2)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = cbam_block(x)\n",
    "    x = residual_block(x, 128)\n",
    "    x3 = MaxPooling2D((2,2))(x)\n",
    "    \n",
    "    x1_aspp = aspp(x1,(112,112),1)\n",
    "    x2_aspp = aspp(x2,(56,56),1)\n",
    "    x3_aspp = aspp(x3,(28,28),1)\n",
    "\n",
    "    ginp1 = UpSampling2D(size=(2, 2), interpolation='bilinear')(x1_aspp)\n",
    "    ginp2 = UpSampling2D(size=(4, 4), interpolation='bilinear')(x2_aspp)\n",
    "    ginp3 = UpSampling2D(size=(8, 8), interpolation='bilinear')(x3_aspp)\n",
    "    \n",
    "    hypercolumn = Concatenate()([ginp1, ginp2, ginp3]) \n",
    "    gap = GlobalAveragePooling2D()(hypercolumn)\n",
    "\n",
    "    x = Dense(256, activation=None)(gap)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dropout_)(x)\n",
    "    \n",
    "    x = Dense(256, activation=None)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    y = Dense(n_classes, activation=\"softmax\", name=\"LiverNet\")(x)\n",
    "   \n",
    "    model = Model(init, y)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### ###### ###### HCNNet ###### ###### ###### \n",
    "\n",
    "# Function for a inception module block\n",
    "def inception_module(layer_in, f1, f2, f3):\n",
    "    # 1x1 conv\n",
    "    conv1 = Conv2D(f1, (1,1), padding='same', activation='relu')(layer_in)\n",
    "    # 3x3 conv\n",
    "    conv3 = Conv2D(f2, (3,3), padding='same', activation='relu')(layer_in)\n",
    "    # 5x5 conv\n",
    "    conv5 = Conv2D(f3, (5,5), padding='same', activation='relu')(layer_in)\n",
    "    # 3x3 max pooling\n",
    "    pool = MaxPooling2D((3,3), strides=(1,1), padding='same')(layer_in)\n",
    "    # concatenate filters, assumes filters/channels last\n",
    "    layer_out = concatenate([conv1, conv3, conv5, pool], axis=-1)\n",
    "    return layer_out\n",
    "\n",
    "def HCNNet(input_W=img_width, input_H= img_height, n_classes=4, dropout_=0.1):\n",
    "    \n",
    "    # define model input\n",
    "    init = Input((input_W,input_H,3))\n",
    "    x = Conv2D(64, (3, 3), activation=None, padding='same')(init) \n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((3,3), strides=(1,1), padding='same')(x)\n",
    "\n",
    "    x = Conv2D(64, (3, 3), activation=None, padding='same')(x) \n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((3,3), strides=(1,1), padding='same')(x)\n",
    "\n",
    "    # add inception module\n",
    "    layer1 = inception_module(x, 64, 128, 32)\n",
    "    layer2 = inception_module(layer1, 128, 192, 96)\n",
    "    layer3 = inception_module(layer2, 192, 208, 48)\n",
    "    layer4 = inception_module(layer3, 160, 224, 64)\n",
    "    layer5 = inception_module(layer4, 128, 256, 64)\n",
    "    layer6 = inception_module(layer5, 112, 288, 64)\n",
    "    layer7 = inception_module(layer6, 256, 320, 128)\n",
    "\n",
    "    x_ = MaxPooling2D((3,3), strides=(1,1), padding='same')(layer7)\n",
    "\n",
    "    layer8 = inception_module(x_, 256, 320, 128)\n",
    "    layer9 = inception_module(layer8, 384, 384, 128)\n",
    "\n",
    "    x_1 = MaxPooling2D((7,7), padding='same')(layer9)\n",
    "\n",
    "    x_2 = Dense(1024, activation='relu')(Flatten()(x_1))\n",
    "\n",
    "    y = Dense(n_classes, activation=\"softmax\", name=\"HCNNet\")(x_2)\n",
    "\n",
    "    model = Model(init, y)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### ###### ###### RuneCNN ###### ###### ###### \n",
    "\n",
    "def encoder(input_img):\n",
    "    #Encoder\n",
    "    x = Conv2D(8, (3, 3), strides=1, activation='relu', padding='same')(input_img)\n",
    "    x = MaxPooling2D((2, 2), strides=1,  padding='same')(x)\n",
    "\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Conv2D(8, (3, 3), strides=2, activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=1, padding='same')(x)\n",
    "\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Conv2D(8, (3, 3), strides=2, activation='relu', padding='same')(x)\n",
    "\n",
    "    x = Dense(4096, activation='relu')(Flatten()(x))\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(2048, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    encoded = Dense(1024, activation='relu')(x)\n",
    "#     print(\"shape of encoded\", K.int_shape(encoded))\n",
    "\n",
    "    return encoded\n",
    "\n",
    "def classifier_C(encode,num_classes,dropout_):\n",
    "    layer_1 = Dense(256, activation='relu')(encode)\n",
    "    layer_2 = Dense(512, activation='relu')(layer_1)\n",
    "    \n",
    "    layer_1 = Dropout(dropout_)(layer_1)\n",
    "    output = Dense(num_classes, activation='softmax', name=\"RuneCNN\")(layer_2)\n",
    "    return output\n",
    "\n",
    "def RuneCNN(input_W=img_width, input_H= img_height, n_classes=4, dropout_=0.1):\n",
    "    \n",
    "    \"\"\"\n",
    "        Method to create the final model\n",
    "    \"\"\"\n",
    "    \n",
    "    input_img = Input((input_W,input_H,3))\n",
    "    encode = encoder(input_img)\n",
    "    model = Model(input_img,classifier_C(encode,n_classes,dropout_))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Select Model Option \n",
      "\n",
      "\n",
      " 1-ResNet \n",
      " 2-DensNet \n",
      " 3-Inception \n",
      " 4-MobileNet \n",
      " 5-RuneCNN  \n",
      " 6-BreastNet \n",
      " 7-LiverNet  \n",
      " 8-HCNNet \n",
      "8\n",
      "\n",
      "Loading Model \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import vgg16, inception_v3, resnet50, mobilenet, densenet\n",
    "\n",
    "print('\\nSelect Model Option \\n')\n",
    "option_DM = int(input(\"\\n 1-ResNet \\n 2-DensNet \\n 3-Inception \\n 4-MobileNet \\n 5-RuneCNN  \\n 6-BreastNet \\n 7-LiverNet  \\n 8-HCNNet \\n\"))\n",
    "while option_DM not in (1,2,3,4,5,6,7,8):\n",
    "    option_DM = int(input(\"\\n 1-ResNet \\n 2-DensNet \\n 3-Inception \\n 4-MobileNet \\n 5-RuneCNN  \\n 6-BreastNet \\n 7-LiverNet  \\n 8-HCNNet \\n\"))\n",
    "\n",
    "with tf.device(\"/device:GPU:0\"):\n",
    "    print('\\nLoading Model \\n')\n",
    "    if option_DM==1:\n",
    "        model = resnet50.ResNet50(weights='imagenet', include_top=False, input_shape= (img_height,img_width,3),classes=num_classes)\n",
    "        filepath = \"weights.best 5FOLD ResNet.hdf5\"\n",
    "\n",
    "        # ResNet\n",
    "        x = Pooling(pooling,model.output)\n",
    "        x = Dense(1024, activation=\"relu\", kernel_regularizer=l2(0.0001))(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "        x = Dense(512, activation=\"relu\", kernel_regularizer=l2(0.0001))(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "        Last_FC_Layer = Dense(num_classes, activation= 'softmax', name='Last_Output_Layer')(x)\n",
    "        \n",
    "    elif option_DM==2:\n",
    "        model = densenet.DenseNet201(weights='imagenet', include_top=False, input_shape= (img_height,img_width,3),classes=num_classes)\n",
    "        filepath = \"weights.best 5FOLD DenseNet.hdf5\"\n",
    "\n",
    "        # DensNet\n",
    "        x = Pooling(pooling,model.output)\n",
    "        x = Dense(1024, activation=\"relu\", kernel_regularizer=l2(0.0001))(x)\n",
    "        x = Dense(512, activation=\"relu\", kernel_regularizer=l2(0.0001))(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "        Last_FC_Layer = Dense(num_classes, activation= 'softmax', name='Last_Output_Layer')(x)\n",
    "        \n",
    "    elif option_DM==3:\n",
    "        model = inception_v3.InceptionV3(weights='imagenet', include_top=False, input_shape= (img_height,img_width,3),classes=num_classes)\n",
    "        filepath = \"weights.best 5FOLD Inception.hdf5\"\n",
    "\n",
    "        # InceptionNet\n",
    "        x = Pooling(pooling,model.output)\n",
    "        x = Dense(1024, activation=\"relu\", kernel_regularizer=l2(0.0001))(x)\n",
    "        x = Dense(512, activation=\"relu\", kernel_regularizer=l2(0.0001))(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "        Last_FC_Layer = Dense(num_classes, activation= 'softmax', name='Last_Output_Layer')(x)\n",
    "\n",
    "    elif option_DM==4:\n",
    "        model = mobilenet.MobileNet(weights='imagenet', include_top=False, input_shape= (img_height,img_width,3),classes=num_classes)\n",
    "        filepath = \"weights.best 5FOLD MobileNet.hdf5\"\n",
    "\n",
    "        # MobileNet\n",
    "        x = Pooling(pooling,model.output)\n",
    "        x = Dense(1024, activation=\"relu\", kernel_regularizer=l2(0.0001))(x)\n",
    "        x = Dense(512, activation=\"relu\", kernel_regularizer=l2(0.0001))(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "        Last_FC_Layer = Dense(num_classes, activation= 'softmax', name='Last_Output_Layer')(x)\n",
    "        \n",
    "    elif option_DM==5:\n",
    "        # RuneCNN\n",
    "        gpu_model = RuneCNN(img_width, img_height, num_classes, dropout)\n",
    "        filepath = \"weights.best 5FOLD RuneCNN.hdf5\"\n",
    "\n",
    "    elif option_DM==6:\n",
    "        # BreastNet\n",
    "        gpu_model = BreastNet(img_width, img_height, num_classes, dropout)\n",
    "        filepath = \"weights.best 5FOLD BreastNet.hdf5\"\n",
    "\n",
    "    elif option_DM==7:\n",
    "        # LiverNet\n",
    "        gpu_model = LiverNet(img_width, img_height, num_classes, dropout)\n",
    "        filepath = \"weights.best 5FOLD LiverNet.hdf5\"\n",
    "\n",
    "    elif option_DM==8:\n",
    "        gpu_model = HCNNet(img_width, img_height, num_classes, dropout)\n",
    "        filepath = \"weights.best 5FOLD HCNNet.hdf5\"\n",
    "\n",
    "#     print('\\nModel Loaded \\n')\n",
    "\n",
    "if option_DM==1 or option_DM==2 or option_DM==3 or option_DM==4:\n",
    "    gpu_model = Model(inputs= model.input, outputs= Last_FC_Layer)\n",
    "    \n",
    "gpu_model.compile(loss=loss_a,optimizer=Optimizer_C,metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " =============== Classifier Parameters =============== \n",
      "\n",
      "Loss:  categorical_crossentropy\n",
      "Optimizer:  <keras.optimizers.Adam object at 0x0000021874D196D8>\n",
      "Batch Size:  128\n",
      "Epoch:  30\n",
      "\n",
      "\n",
      " =============== Train Data Dimensions =============== \n",
      "\n",
      "Features R:  (179046, 4800)\n",
      "Feature Labels:  (179046,)\n",
      "\n",
      "\n",
      " =============== Test Data Dimensions =============== \n",
      "\n",
      "Features R:  (37694, 40, 40, 3)\n"
     ]
    }
   ],
   "source": [
    "# ORIGINAL ONE\n",
    "print('\\n\\n =============== Classifier Parameters =============== ')\n",
    "print('\\nLoss: ', loss_a)\n",
    "print('Optimizer: ', Optimizer_C)\n",
    "print('Batch Size: ', Batch_Size)\n",
    "print('Epoch: ', Epochs_C)\n",
    "\n",
    "print('\\n\\n =============== Train Data Dimensions =============== ')\n",
    "\n",
    "X_trainS,Y_trainS = X_train_R, Y_train_R\n",
    "print('\\nFeatures R: ',X_train_R.shape)\n",
    "print('Feature Labels: ',Y_trainS.shape)\n",
    "\n",
    "print('\\n\\n =============== Test Data Dimensions =============== ')\n",
    "X_testS,Y_testS = X_test_R_T, Y_test_R_T\n",
    "print('\\nFeatures R: ',X_test_R_T.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\t\t =============== Model Summary =============== \n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_14 (InputLayer)           (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_344 (Conv2D)             (None, 224, 224, 64) 1792        input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 224, 224, 64) 0           conv2d_344[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_146 (MaxPooling2D (None, 224, 224, 64) 0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_345 (Conv2D)             (None, 224, 224, 64) 36928       max_pooling2d_146[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 224, 224, 64) 0           conv2d_345[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_147 (MaxPooling2D (None, 224, 224, 64) 0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_346 (Conv2D)             (None, 224, 224, 64) 4160        max_pooling2d_147[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_347 (Conv2D)             (None, 224, 224, 128 73856       max_pooling2d_147[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_348 (Conv2D)             (None, 224, 224, 32) 51232       max_pooling2d_147[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_148 (MaxPooling2D (None, 224, 224, 64) 0           max_pooling2d_147[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_104 (Concatenate)   (None, 224, 224, 288 0           conv2d_346[0][0]                 \n",
      "                                                                 conv2d_347[0][0]                 \n",
      "                                                                 conv2d_348[0][0]                 \n",
      "                                                                 max_pooling2d_148[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_349 (Conv2D)             (None, 224, 224, 128 36992       concatenate_104[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_350 (Conv2D)             (None, 224, 224, 192 497856      concatenate_104[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_351 (Conv2D)             (None, 224, 224, 96) 691296      concatenate_104[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_149 (MaxPooling2D (None, 224, 224, 288 0           concatenate_104[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_105 (Concatenate)   (None, 224, 224, 704 0           conv2d_349[0][0]                 \n",
      "                                                                 conv2d_350[0][0]                 \n",
      "                                                                 conv2d_351[0][0]                 \n",
      "                                                                 max_pooling2d_149[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_352 (Conv2D)             (None, 224, 224, 192 135360      concatenate_105[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_353 (Conv2D)             (None, 224, 224, 208 1318096     concatenate_105[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_354 (Conv2D)             (None, 224, 224, 48) 844848      concatenate_105[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_150 (MaxPooling2D (None, 224, 224, 704 0           concatenate_105[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_106 (Concatenate)   (None, 224, 224, 115 0           conv2d_352[0][0]                 \n",
      "                                                                 conv2d_353[0][0]                 \n",
      "                                                                 conv2d_354[0][0]                 \n",
      "                                                                 max_pooling2d_150[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_355 (Conv2D)             (None, 224, 224, 160 184480      concatenate_106[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_356 (Conv2D)             (None, 224, 224, 224 2322656     concatenate_106[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_357 (Conv2D)             (None, 224, 224, 64) 1843264     concatenate_106[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_151 (MaxPooling2D (None, 224, 224, 115 0           concatenate_106[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_107 (Concatenate)   (None, 224, 224, 160 0           conv2d_355[0][0]                 \n",
      "                                                                 conv2d_356[0][0]                 \n",
      "                                                                 conv2d_357[0][0]                 \n",
      "                                                                 max_pooling2d_151[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_358 (Conv2D)             (None, 224, 224, 128 204928      concatenate_107[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_359 (Conv2D)             (None, 224, 224, 256 3686656     concatenate_107[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_360 (Conv2D)             (None, 224, 224, 64) 2560064     concatenate_107[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_152 (MaxPooling2D (None, 224, 224, 160 0           concatenate_107[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_108 (Concatenate)   (None, 224, 224, 204 0           conv2d_358[0][0]                 \n",
      "                                                                 conv2d_359[0][0]                 \n",
      "                                                                 conv2d_360[0][0]                 \n",
      "                                                                 max_pooling2d_152[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_361 (Conv2D)             (None, 224, 224, 112 229488      concatenate_108[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_362 (Conv2D)             (None, 224, 224, 288 5308704     concatenate_108[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_363 (Conv2D)             (None, 224, 224, 64) 3276864     concatenate_108[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_153 (MaxPooling2D (None, 224, 224, 204 0           concatenate_108[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_109 (Concatenate)   (None, 224, 224, 251 0           conv2d_361[0][0]                 \n",
      "                                                                 conv2d_362[0][0]                 \n",
      "                                                                 conv2d_363[0][0]                 \n",
      "                                                                 max_pooling2d_153[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_364 (Conv2D)             (None, 224, 224, 256 643328      concatenate_109[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_365 (Conv2D)             (None, 224, 224, 320 7234880     concatenate_109[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_366 (Conv2D)             (None, 224, 224, 128 8038528     concatenate_109[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_154 (MaxPooling2D (None, 224, 224, 251 0           concatenate_109[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_110 (Concatenate)   (None, 224, 224, 321 0           conv2d_364[0][0]                 \n",
      "                                                                 conv2d_365[0][0]                 \n",
      "                                                                 conv2d_366[0][0]                 \n",
      "                                                                 max_pooling2d_154[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_155 (MaxPooling2D (None, 224, 224, 321 0           concatenate_110[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_367 (Conv2D)             (None, 224, 224, 256 823552      max_pooling2d_155[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_368 (Conv2D)             (None, 224, 224, 320 9262400     max_pooling2d_155[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_369 (Conv2D)             (None, 224, 224, 128 10291328    max_pooling2d_155[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_156 (MaxPooling2D (None, 224, 224, 321 0           max_pooling2d_155[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_111 (Concatenate)   (None, 224, 224, 392 0           conv2d_367[0][0]                 \n",
      "                                                                 conv2d_368[0][0]                 \n",
      "                                                                 conv2d_369[0][0]                 \n",
      "                                                                 max_pooling2d_156[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_370 (Conv2D)             (None, 224, 224, 384 1505664     concatenate_111[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_371 (Conv2D)             (None, 224, 224, 384 13547904    concatenate_111[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_372 (Conv2D)             (None, 224, 224, 128 12544128    concatenate_111[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_157 (MaxPooling2D (None, 224, 224, 392 0           concatenate_111[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_112 (Concatenate)   (None, 224, 224, 481 0           conv2d_370[0][0]                 \n",
      "                                                                 conv2d_371[0][0]                 \n",
      "                                                                 conv2d_372[0][0]                 \n",
      "                                                                 max_pooling2d_157[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_158 (MaxPooling2D (None, 32, 32, 4816) 0           concatenate_112[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 4931584)      0           max_pooling2d_158[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 1024)         754975744   flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "HCNNet (Dense)                  (None, 5)            5125        dense_17[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 842,182,101\n",
      "Trainable params: 842,182,101\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n\\t\\t =============== Model Summary =============== ')\n",
    "print(gpu_model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_history = []  \n",
    "cv_scores = []  \n",
    "\n",
    "skf = StratifiedKFold(n_splits=kfold_splits, shuffle=True)\n",
    "\n",
    "# Learning Rate Reducer\n",
    "learn_control = ReduceLROnPlateau(monitor=monitor, patience= patience,verbose=verbose,factor=factor, min_lr=min_lr)\n",
    "\n",
    "if not (os.path.exists(filepath)):\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor=monitor, verbose=verbose, save_best_only=True, mode=mode)\n",
    "    print('\\nNot Using real-time data augmentation.')\n",
    "    print('Training Starts...')      \n",
    "    print('X_train Concatenate shape:', X_trainS.shape)\n",
    "    print('Y_train Concatenate shape:', Y_trainS.shape)                                                                                                                                                                      \n",
    "\n",
    "    for index, (train_indices, val_indices) in enumerate(skf.split(X_trainS,Y_trainS)):\n",
    "        print(\"\\n\\n\\tTraining on fold \" + str(index+1) + \"/\" + str(kfold_splits) + \"...\")\n",
    "        xtrain_R, xvalid_R = X_trainS[train_indices], X_trainS[val_indices]\n",
    "        ytrain_R, yvalid_R = Y_trainS[train_indices], Y_trainS[val_indices]    \n",
    "\n",
    "        ytrain = tf.keras.utils.to_categorical(ytrain_R, num_classes)\n",
    "        yvalid = tf.keras.utils.to_categorical(yvalid_R, num_classes)        \n",
    "\n",
    "        print('\\nAfter Applying Categorical Opeartion')\n",
    "        print('Train data:', xtrain_R.shape,', Train labels:', ytrain.shape)\n",
    "        print('Validation data:', xvalid_R.shape,', Validation labels:', yvalid.shape)\n",
    "\n",
    "#                     print(\"Mode: \", Mode)\n",
    "        with tf.device('/device:GPU:0'):\n",
    "            history = gpu_model.fit(xtrain_R, ytrain, batch_size=Batch_Size, epochs=Epochs_C, validation_data=(xvalid_R, yvalid), verbose=1, callbacks=[learn_control, checkpoint])\n",
    "        model_history.append(history)    \n",
    "        print('\\nEvaluating the Model...')\n",
    "        scores = gpu_model.evaluate(xvalid_R, yvalid,verbose=0)\n",
    "        print(\"%s: %.2f%%\" % (gpu_model.metrics_names[1], scores[1]*100))\n",
    "        cv_scores.append(scores[1] * 100)\n",
    "\n",
    "        del xtrain_R, xvalid_R, ytrain, yvalid, ytrain_R, yvalid_R     \n",
    "\n",
    "    print(\"\\n%s: %.2f%%\" % (\"Mean Accuracy: \",np.mean(cv_scores)))\n",
    "    print(\"%s: %.2f%%\" % (\"Standard Deviation: +/-\", np.std(cv_scores)))\n",
    "    print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cv_scores), np.std(cv_scores)))\n",
    "    print('Estimated Accuracy %.3f (%.3f)' % (np.mean(cv_scores), np.std(cv_scores)))\n",
    "\n",
    "else:\n",
    "# load the models\n",
    "    print('\\n Classifier Loading Saved Models')\n",
    "    print('Model : ',filepath)\n",
    "    gpu_model = load_model(filepath)\n",
    "\n",
    "print(\"\\n\")                \n",
    "Y_testa_Old = Y_testS\n",
    "print('Before Categorical Features : ',Y_testa_Old.shape)\n",
    "\n",
    "Y_testS_ = tf.keras.utils.to_categorical(Y_testS, num_classes)      \n",
    "print('After Categorical Features : ',Y_testS_.shape)\n",
    "\n",
    "test_eval = gpu_model.evaluate(X_test_R_T,Y_testS_, verbose=0)\n",
    "\n",
    "print('\\n\\n =============== Overall Average Results =============== ')\n",
    "print('\\nAccuracy:', test_eval[1])\n",
    "\n",
    "#Predicted Labels\n",
    "predicted_classes = gpu_model.predict(X_test_R_T)\n",
    "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
    "test_labels = Y_testS\n",
    "\n",
    "print(\"Sensitivity = \", recall_score(test_labels, predicted_classes, average='weighted'))\n",
    "print(\"Precision = \", precision_score(test_labels, predicted_classes, average='weighted'))\n",
    "print(\"F1 = \", f1_score(test_labels, predicted_classes, average='weighted'))\n",
    "\n",
    "\n",
    "print('\\n\\n =============== Classwise Results =============== ')\n",
    "cmat = confusion_matrix(test_labels, predicted_classes) \n",
    "print(\"\\nAccuracy = \", cmat.diagonal()/cmat.sum(axis=1))\n",
    "print(\"Sensitivity = \", recall_score(test_labels, predicted_classes, average=None))\n",
    "print(\"Precision = \", precision_score(test_labels, predicted_classes, average=None))\n",
    "print(\"F1 = \", f1_score(test_labels, predicted_classes, average=None))\n",
    "\n",
    "\n",
    "print(\"\\n\\n\")            \n",
    "target_names = [\"Class {}\".format(ia) for ia in range(num_classes)]\n",
    "print(classification_report(test_labels, predicted_classes, target_names=target_names))\n",
    "\n",
    "\n",
    "with open(Path_S_1 + \"/Metrics_Result Others ICAIR.txt\", \"a\") as myfile:\n",
    "\n",
    "    myfile.write(\"\\n\\n =============== Dataset_\" + str(option_D0a) + \"=============== \")\n",
    "#     myfile.write(\"\\nAE Configuration: No. of Layers = \" + str(Number_of_Hidden_Layers_Input) + \", No. of Neurons = \" + str(Number_of_Neurons_Input) + \", Activation No = \" + str(Activation_Function_Input) + \"\\n\")                    \n",
    "    myfile.write(\"\\n =============== Overall Average Results =============== \")\n",
    "    myfile.write(\"\\nAccuracy: \" + str(\"%.5f\" % test_eval[1]))\n",
    "    myfile.write(\"\\nLoss: \" + str(\"%.5f\" % test_eval[0]))\n",
    "    myfile.write(\"\\nSensitivity: \" + str(recall_score(test_labels, predicted_classes, average='weighted')))\n",
    "    myfile.write(\"\\nPrecision: \" + str(precision_score(test_labels, predicted_classes, average='weighted')))\n",
    "    myfile.write(\"\\nF-1_Score: \" + str(f1_score(test_labels, predicted_classes, average='weighted')) + \"\\n\")\n",
    "\n",
    "    myfile.write(\"\\n =============== Classwise Average Results =============== \")\n",
    "    myfile.write(\"\\nAccuracy: \" + str(cmat.diagonal()/cmat.sum(axis=1)))\n",
    "    myfile.write(\"\\nSensitivity: \" + str(recall_score(test_labels, predicted_classes, average=None)))\n",
    "    myfile.write(\"\\nPrecision: \" + str(precision_score(test_labels, predicted_classes, average=None)))\n",
    "    myfile.write(\"\\nF-1_Score: \" + str(f1_score(test_labels, predicted_classes, average=None)) + \"\\n\")\n",
    "\n",
    "columns = 16\n",
    "work_directory_P = os.getcwd() + \"/ResultsPredicted\"\n",
    "\n",
    "if not os.path.exists(work_directory_P):\n",
    "    os.mkdir(work_directory_P)\n",
    "\n",
    "fig=plt.figure(figsize=(8, 8))\n",
    "\n",
    "left  = 0.125  # the left side of the subplots of the figure\n",
    "right = 1    # the right side of the subplots of the figure\n",
    "bottom = 0.1   # the bottom of the subplots of the figure\n",
    "top = 0.9      # the top of the subplots of the figure\n",
    "wspace = 0.25   # the amount of width reserved for blank space between subplots\n",
    "hspace = 0.25   # the amount of height reserved for white space between subplots\n",
    "\n",
    "plt.subplots_adjust( left, bottom, right, top, wspace, hspace)\n",
    "columns = 5\n",
    "rows = 5\n",
    "f = 12\n",
    "print('\\n\\n =============== Correct Predicted Samples =============== ')\n",
    "\n",
    "correct = np.where(predicted_classes==test_labels)[0]\n",
    "print(\"\\nFound %d correct labels\" % len(correct))\n",
    "for i1, correct in enumerate(correct[:rows*columns]):\n",
    "    ax = fig.add_subplot(rows, columns, i1+1)\n",
    "    plt.imshow(X_testaB[correct].astype('uint8'))\n",
    "    plt.axis('off')        \n",
    "    ax.set_title(\"Predict {}, Class {}\".format(predicted_classes[correct], test_labels[correct]), fontsize=f)\n",
    "    plt.imsave(work_directory_P+'/Correc_D-'+ str(option_D0a) + '_PL-' + str(predicted_classes[correct]) + '_TL-' + str(test_labels[correct]) + '_I-' + str(i1) + '.jpg', X_testaB[correct].astype('uint8'), dpi=fig.dpi)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig=plt.figure(figsize=(8, 8))\n",
    "plt.subplots_adjust( left, bottom, right, top, wspace, hspace)\n",
    "\n",
    "print('\\n\\n =============== Incorrect Predicted Samples =============== ')\n",
    "\n",
    "incorrect = np.where(predicted_classes!=test_labels)[0]\n",
    "print(\"\\nFound %d incorrect labels\" % len(incorrect))\n",
    "for i2, incorrect in enumerate(incorrect[:rows*columns]):\n",
    "    ax = fig.add_subplot(rows, columns, i2+1)\n",
    "    plt.imshow(X_testaB[incorrect].astype('uint8'))\n",
    "    plt.axis('off')        \n",
    "    ax.set_title(\"Predict {}, Class {}\".format(predicted_classes[incorrect], test_labels[incorrect]), fontsize=f)\n",
    "    plt.imsave(work_directory_P+'/Incorrect_D-'+ str(option_D0a) + '_PL-' + str(predicted_classes[incorrect]) + '_TL-' + str(test_labels[incorrect]) + '_I-' + str(i2) + '.jpg', X_testaB[incorrect].astype('uint8'), dpi=fig.dpi)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\n\\n ===============  Results =============== ')\n",
    "\n",
    "y_score = gpu_model.predict([X_test_R_T,X_test_H_T,X_test_L_T])\n",
    "y_test = tf.keras.utils.to_categorical(Y_testS, num_classes)      \n",
    "print(y_score.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# Plot linewidth.\n",
    "lw = 1\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for ia in range(num_classes):\n",
    "    fpr[ia], tpr[ia], _ = roc_curve(y_test[:, ia], y_score[:, ia])\n",
    "    roc_auc[ia] = auc(fpr[ia], tpr[ia])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "all_fpr = np.unique(np.concatenate([fpr[ib] for ib in range(num_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for ic in range(num_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[ic], tpr[ic])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= num_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure(1)\n",
    "plt.style.use('default')    \n",
    "plt.rcParams.update({'font.size': 13})\n",
    "\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='Micro Average (Area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=3)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='Macro Average (Area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=3)\n",
    "\n",
    "\n",
    "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "colors = cycle(prop_cycle.by_key()['color'])\n",
    "for id, color in zip(range(num_classes), colors):\n",
    "    if id==0:\n",
    "        plt.plot(fpr[id], tpr[id], color=next(colors), lw=lw,\n",
    "                 label='Class {0} (Area = {1:0.2f})'\n",
    "                 ''.format('A', roc_auc[id]))\n",
    "    elif id==1:\n",
    "        plt.plot(fpr[id], tpr[id], color=next(colors), lw=lw,\n",
    "                 label='Class {0} (Area = {1:0.2f})'\n",
    "                 ''.format('L', roc_auc[id]))\n",
    "    elif id==2:\n",
    "        plt.plot(fpr[id], tpr[id], color=next(colors), lw=lw,\n",
    "                 label='Class {0} (Area = {1:0.2f})'\n",
    "                 ''.format('M', roc_auc[id]))\n",
    "    elif id==3:\n",
    "        plt.plot(fpr[id], tpr[id], color=next(colors), lw=lw,\n",
    "                 label='Class {0} (Area = {1:0.2f})'\n",
    "                 ''.format('P', roc_auc[id]))\n",
    "    elif id==4:\n",
    "        plt.plot(fpr[id], tpr[id], color=next(colors), lw=lw,\n",
    "                 label='Class {0} (Area = {1:0.2f})'\n",
    "                 ''.format('S', roc_auc[id]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([-0.10, 1.10])\n",
    "plt.ylim([-0.10, 1.10])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.75, 1.0))\n",
    "\n",
    "#             work_directory_R = os.getcwd() + \"/Results_ROC\"\n",
    "#             work_directory_C = os.getcwd() + \"/Results_CM\"\n",
    "\n",
    "#             if not os.path.exists(work_directory_R):\n",
    "#                 os.mkdir(work_directory_R)\n",
    "\n",
    "#             plt.savefig(work_directory_R + '/ROC_D' + str(option_D0a), dpi=1000, bbox_inches=\"tight\")\n",
    "#             plt.show()\n",
    "\n",
    "#             if not os.path.exists(work_directory_C):\n",
    "#                 os.mkdir(work_directory_C)\n",
    "\n",
    "#             normalize = False\n",
    "#             plot_cm(test_labels, predicted_classes, option_D0a, normalize, work_directory_C)\n",
    "\n",
    "del gpu_model, predicted_classes, test_labels\n",
    "# ORIGINAL CODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
